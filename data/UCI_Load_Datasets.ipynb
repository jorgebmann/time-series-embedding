{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Directory where the dataset will be downloaded and extracted\n",
        "DATA_DIR = 'datasets'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def download_dataset(dataset_name, url):\n",
        "    \"\"\"\n",
        "    Downloads and extracts a zip file containing the dataset.\n",
        "    \"\"\"\n",
        "    zip_path = os.path.join(DATA_DIR, f\"{dataset_name}.zip\")\n",
        "    extract_path = os.path.join(DATA_DIR, dataset_name)\n",
        "\n",
        "    # Download the dataset\n",
        "    print(f\"Downloading {dataset_name} from {url}...\")\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "    # Extract the zip file\n",
        "    print(f\"Extracting {dataset_name}...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "        file_names = zip_ref.namelist()\n",
        "        print(f\"Files in the zip: {file_names}\")\n",
        "\n",
        "    # Remove the zip file after extraction\n",
        "    os.remove(zip_path)\n",
        "    print(f\"Dataset {dataset_name} extracted to {extract_path}.\")\n",
        "    return extract_path, file_names\n",
        "\n",
        "def load_data(file_path, encoding='ISO-8859-1'):\n",
        "    \"\"\"\n",
        "    Loads the dataset from a CSV file and returns a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    print(f\"Loading data from {file_path}\")\n",
        "    df = pd.read_csv(file_path, encoding=encoding, on_bad_lines='skip')\n",
        "    return df\n",
        "\n",
        "def preprocess_data(df, batch_size=64, time_steps=30):\n",
        "    \"\"\"\n",
        "    Preprocesses the data:\n",
        "    - Drops Date and Time columns.\n",
        "    - Uses all remaining features.\n",
        "    - Normalizes the features.\n",
        "    - Reshapes data into 3D tensors (samples, time_steps, features).\n",
        "    - Splits into training, validation, and test sets.\n",
        "    \"\"\"\n",
        "    print(\"Starting preprocessing...\")\n",
        "\n",
        "    # Drop Date and Time columns\n",
        "    df = df.drop(columns=['Date', 'Time'])\n",
        "    print(\"Dropped Date and Time columns.\")\n",
        "\n",
        "    # Use all remaining columns as features\n",
        "    X = df.drop(columns=['Room_Occupancy_Count'])\n",
        "    y = df['Room_Occupancy_Count']\n",
        "    features = X.shape[1]  # Set features dynamically based on remaining columns\n",
        "    print(f\"Total features used: {features}\")\n",
        "\n",
        "    # Ensure total samples are compatible with reshaping\n",
        "    total_samples = (len(X) // time_steps) * time_steps\n",
        "    X, y = X.iloc[:total_samples], y.iloc[:total_samples]\n",
        "    print(f\"Adjusted dataset size: {X.shape}, Labels size: {y.shape}\")\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    print(\"Features normalized.\")\n",
        "\n",
        "    # Reshape X into (samples, time_steps, features)\n",
        "    X = X.reshape(-1, time_steps, features)\n",
        "    y = y.values.reshape(-1, time_steps)[:, 0]  # Ensure y matches the number of sequences in X\n",
        "    print(f\"Reshaped X to: {X.shape}, Reshaped y to: {y.shape}\")\n",
        "\n",
        "    # Split into training, validation, and test sets\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "    print(\"Data split into training, validation, and test sets.\")\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.int64)\n",
        "\n",
        "    X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
        "    y_valid = torch.tensor(y_valid, dtype=torch.int64)\n",
        "\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.int64)\n",
        "\n",
        "    # Output dataset shapes\n",
        "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "    print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "    print(\"Preprocessing complete.\")\n",
        "    return train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test\n",
        "\n",
        "# Main function to run the entire process\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_name = 'RoomOccupancy'\n",
        "    dataset_url = 'https://archive.ics.uci.edu/static/public/864/room+occupancy+estimation.zip'\n",
        "\n",
        "    # Download and extract the dataset\n",
        "    extract_path, file_names = download_dataset(dataset_name, dataset_url)\n",
        "\n",
        "    # Load the data\n",
        "    main_file_path = os.path.join(extract_path, file_names[0])  # Assuming the first file is the main CSV\n",
        "    df = load_data(main_file_path)\n",
        "\n",
        "    # Preprocess the data\n",
        "    train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test = preprocess_data(df)\n",
        "\n",
        "    # Output the number of classes\n",
        "    n_classes = len(torch.unique(y_train))\n",
        "    print(f\"Number of classes: {n_classes}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdDd4yuJmlF0",
        "outputId": "9448511c-e01f-4a01-de30-57ad528cb4cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading RoomOccupancy from https://archive.ics.uci.edu/static/public/864/room+occupancy+estimation.zip...\n",
            "Extracting RoomOccupancy...\n",
            "Files in the zip: ['Occupancy_Estimation.csv']\n",
            "Dataset RoomOccupancy extracted to datasets/RoomOccupancy.\n",
            "Loading data from datasets/RoomOccupancy/Occupancy_Estimation.csv\n",
            "Starting preprocessing...\n",
            "Dropped Date and Time columns.\n",
            "Total features used: 16\n",
            "Adjusted dataset size: (10110, 16), Labels size: (10110,)\n",
            "Features normalized.\n",
            "Reshaped X to: (337, 30, 16), Reshaped y to: (337,)\n",
            "Data split into training, validation, and test sets.\n",
            "X_train shape: torch.Size([202, 30, 16]), y_train shape: torch.Size([202])\n",
            "X_valid shape: torch.Size([67, 30, 16]), y_valid shape: torch.Size([67])\n",
            "X_test shape: torch.Size([68, 30, 16]), y_test shape: torch.Size([68])\n",
            "Preprocessing complete.\n",
            "Number of classes: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Directory where datasets will be downloaded and extracted\n",
        "DATA_DIR = 'datasets'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def download_dataset(dataset_name, url):\n",
        "    \"\"\"\n",
        "    Downloads and extracts a zip file containing the dataset.\n",
        "    \"\"\"\n",
        "    zip_path = os.path.join(DATA_DIR, f\"{dataset_name}.zip\")\n",
        "    extract_path = os.path.join(DATA_DIR, dataset_name)\n",
        "\n",
        "    # Download the dataset\n",
        "    print(\"Starting download...\")\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "    print(\"Download complete.\")\n",
        "\n",
        "    # Extract the zip file\n",
        "    print(\"Extracting dataset...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    # Remove the zip file after extraction\n",
        "    os.remove(zip_path)\n",
        "    print(\"Extraction complete.\")\n",
        "    return extract_path\n",
        "\n",
        "def load_emg_data(directory):\n",
        "    \"\"\"\n",
        "    Loads and cleans EMG data by reading files, skipping metadata rows, and selecting numeric data.\n",
        "    Assumes that each file is in a plain text format with space-delimited values.\n",
        "    \"\"\"\n",
        "    data_frames = []\n",
        "\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file_name in files:\n",
        "            if file_name.endswith('.txt'):\n",
        "                file_path = os.path.join(root, file_name)\n",
        "                try:\n",
        "                    # Read the file, skipping rows with metadata or headers\n",
        "                    df = pd.read_csv(file_path, sep='\\s+', header=None, skiprows=3)\n",
        "\n",
        "                    # Check if the DataFrame has valid numeric content\n",
        "                    if df.empty or not pd.to_numeric(df.iloc[:, 1], errors='coerce').notna().all():\n",
        "                        continue\n",
        "\n",
        "                    # Convert all data to numeric\n",
        "                    df = df.apply(pd.to_numeric, errors='coerce')  # Ensure all data is numeric\n",
        "                    data_frames.append(df)\n",
        "                except pd.errors.ParserError:\n",
        "                    continue\n",
        "\n",
        "    if not data_frames:\n",
        "        raise ValueError(\"No valid numeric data files found for concatenation.\")\n",
        "\n",
        "    # Concatenate and reset index\n",
        "    full_df = pd.concat(data_frames, axis=0).reset_index(drop=True)\n",
        "    full_df.columns = [f'feature_{i}' for i in range(full_df.shape[1] - 1)] + ['label']  # Assign last column as label\n",
        "\n",
        "    print(\"Data loaded successfully.\")\n",
        "    return full_df\n",
        "\n",
        "def preprocess_data(df, batch_size=64, time_steps=30):\n",
        "    \"\"\"\n",
        "    Preprocesses the EMG data:\n",
        "    - Reshapes the dataset into sequences with time steps and features.\n",
        "    - Splits into train, validation, and test sets.\n",
        "    - Normalizes the features.\n",
        "    - Converts them into PyTorch tensors.\n",
        "    - Creates DataLoaders for supervised tasks (with labels).\n",
        "    \"\"\"\n",
        "    print(\"Starting preprocessing...\")\n",
        "    # Assume the last column is the label and the rest are features\n",
        "    X = df.iloc[:, :-1]  # All columns except the last\n",
        "    y = df.iloc[:, -1]   # Last column as labels\n",
        "    num_features = X.shape[1]\n",
        "\n",
        "    # Adjust the data to make it divisible by the number of time steps\n",
        "    total_samples = (len(X) // time_steps) * time_steps\n",
        "    X, y = X.iloc[:total_samples], y.iloc[:total_samples]\n",
        "\n",
        "    # Reshape X into (num_sequences, time_steps, num_features)\n",
        "    X = X.values.reshape(-1, time_steps, num_features)\n",
        "    y = y.values.reshape(-1, time_steps)[:, 0]  # Use the first label of each sequence as the label for that sequence\n",
        "\n",
        "    # Split into training, validation, and test sets\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train.reshape(-1, num_features)).reshape(-1, time_steps, num_features)\n",
        "    X_valid = scaler.transform(X_valid.reshape(-1, num_features)).reshape(-1, time_steps, num_features)\n",
        "    X_test = scaler.transform(X_test.reshape(-1, num_features)).reshape(-1, time_steps, num_features)\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "    X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
        "    y_valid = torch.tensor(y_valid, dtype=torch.long)\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "    # Print the shapes of the datasets\n",
        "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "    print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "    print(f\"\\nEach set details:\")\n",
        "    print(f\"- Training set: {X_train.shape[0]} sequences, {time_steps} time steps, {num_features} features per step.\")\n",
        "    print(f\"- Validation set: {X_valid.shape[0]} sequences, {time_steps} time steps, {num_features} features per step.\")\n",
        "    print(f\"- Test set: {X_test.shape[0]} sequences, {time_steps} time steps, {num_features} features per step.\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    valid_loader = DataLoader(TensorDataset(X_valid, y_valid), batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test\n",
        "\n",
        "def main():\n",
        "    dataset_name = 'EMG_Gestures'\n",
        "    dataset_url = 'https://archive.ics.uci.edu/static/public/481/emg+data+for+gestures.zip'\n",
        "\n",
        "    # Step 1: Download and Extract\n",
        "    extract_path = download_dataset(dataset_name, dataset_url)\n",
        "\n",
        "    # Step 2: Load Data\n",
        "    df = load_emg_data(extract_path)  # Ensure labels are in the last column\n",
        "\n",
        "    # Step 3: Preprocess Data into Training, Validation, and Testing sets\n",
        "    train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test = preprocess_data(df)\n",
        "\n",
        "    # Step 4: Output the Number of Classes (If Labels are present)\n",
        "    n_classes = len(torch.unique(y_train))\n",
        "    print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9Jk9dC7rvn7",
        "outputId": "077fd17d-d1f1-4daf-cbf6-aa50a94ee948"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting download...\n",
            "Download complete.\n",
            "Extracting dataset...\n",
            "Extraction complete.\n",
            "Data loaded successfully.\n",
            "Starting preprocessing...\n",
            "X_train shape: torch.Size([84754, 30, 9]), y_train shape: torch.Size([84754])\n",
            "X_valid shape: torch.Size([28252, 30, 9]), y_valid shape: torch.Size([28252])\n",
            "X_test shape: torch.Size([28252, 30, 9]), y_test shape: torch.Size([28252])\n",
            "\n",
            "Each set details:\n",
            "- Training set: 84754 sequences, 30 time steps, 9 features per step.\n",
            "- Validation set: 28252 sequences, 30 time steps, 9 features per step.\n",
            "- Test set: 28252 sequences, 30 time steps, 9 features per step.\n",
            "Number of classes: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Directory where the dataset will be downloaded and extracted\n",
        "DATA_DIR = 'datasets'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def download_dataset(dataset_name, url):\n",
        "    \"\"\"\n",
        "    Downloads and extracts a zip file containing the dataset.\n",
        "    \"\"\"\n",
        "    zip_path = os.path.join(DATA_DIR, f\"{dataset_name}.zip\")\n",
        "    extract_path = os.path.join(DATA_DIR, dataset_name)\n",
        "\n",
        "    # Download the dataset\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    # Remove the zip file after extraction\n",
        "    os.remove(zip_path)\n",
        "    return extract_path\n",
        "\n",
        "def load_data(directory):\n",
        "    \"\"\"\n",
        "    Loads and cleans the dataset from text files in the specified directory.\n",
        "    \"\"\"\n",
        "    data_frames = []\n",
        "\n",
        "    for root, _, files in os.walk(directory):\n",
        "        for file_name in files:\n",
        "            if file_name.endswith('.txt'):\n",
        "                file_path = os.path.join(root, file_name)\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path, delimiter=';', header=None, on_bad_lines='skip')\n",
        "                    df = df.apply(pd.to_numeric, errors='coerce').dropna()\n",
        "                    data_frames.append(df)\n",
        "                except pd.errors.ParserError:\n",
        "                    continue\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "    if not data_frames:\n",
        "        raise ValueError(\"No valid data files found for concatenation.\")\n",
        "\n",
        "    full_df = pd.concat(data_frames, axis=0).reset_index(drop=True)\n",
        "    X = full_df.iloc[:, :-1]\n",
        "    y = full_df.iloc[:, -1]\n",
        "    return X, y\n",
        "\n",
        "def preprocess_data(X, y, batch_size=64, time_steps=30):\n",
        "    \"\"\"\n",
        "    Preprocesses the data:\n",
        "    - Reshapes the dataset into sequences with time steps and features.\n",
        "    - Normalizes the features.\n",
        "    - Splits into training, validation, and test sets.\n",
        "    - Converts them into PyTorch tensors.\n",
        "    - Creates DataLoaders for training, validation, and testing.\n",
        "    \"\"\"\n",
        "    num_features = X.shape[1]\n",
        "\n",
        "    # Adjust dataset size to ensure it's divisible by the time steps\n",
        "    total_samples = (len(X) // time_steps) * time_steps\n",
        "    X, y = X.iloc[:total_samples], y.iloc[:total_samples]\n",
        "\n",
        "    # Reshape X into (num_sequences, time_steps, num_features)\n",
        "    X = X.values.reshape(-1, time_steps, num_features)\n",
        "    y = y.values.reshape(-1, time_steps)[:, 0]  # Use the first label of each sequence as the sequence label\n",
        "\n",
        "    # Split the data into train, validation, and test sets\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "    X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train.reshape(-1, num_features)).reshape(-1, time_steps, num_features)\n",
        "    X_valid = scaler.transform(X_valid.reshape(-1, num_features)).reshape(-1, time_steps, num_features)\n",
        "    X_test = scaler.transform(X_test.reshape(-1, num_features)).reshape(-1, time_steps, num_features)\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "    X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
        "    y_valid = torch.tensor(y_valid, dtype=torch.long)\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "    # Print the shapes and structured details\n",
        "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "    print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "    print(f\"\\nEach set details:\")\n",
        "    print(f\"- Training set: {X_train.shape[0]} sequences, {time_steps} time steps, {num_features} features per step.\")\n",
        "    print(f\"- Validation set: {X_valid.shape[0]} sequences, {time_steps} time steps, {num_features} features per step.\")\n",
        "    print(f\"- Test set: {X_test.shape[0]} sequences, {time_steps} time steps, {num_features} features per step.\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    valid_loader = DataLoader(TensorDataset(X_valid, y_valid), batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test\n",
        "\n",
        "# Main function to run the entire process\n",
        "if __name__ == \"__main__\":\n",
        "    dataset_name = 'PhysicalTherapyExercises'\n",
        "    dataset_url = 'https://archive.ics.uci.edu/static/public/730/physical+therapy+exercises+dataset.zip'\n",
        "\n",
        "    # Download and extract the dataset\n",
        "    extract_path = download_dataset(dataset_name, dataset_url)\n",
        "\n",
        "    # Load the data\n",
        "    X, y = load_data(extract_path)\n",
        "\n",
        "    # Preprocess the data\n",
        "    train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test = preprocess_data(X, y)\n",
        "\n",
        "    # Output the number of classes\n",
        "    n_classes = len(torch.unique(y_train))\n",
        "    print(f\"Number of classes: {n_classes}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZiJ7HryskQ8",
        "outputId": "0cae0883-14e4-458e-9e5c-64995148e4dd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: torch.Size([27562, 30, 9]), y_train shape: torch.Size([27562])\n",
            "X_valid shape: torch.Size([9187, 30, 9]), y_valid shape: torch.Size([9187])\n",
            "X_test shape: torch.Size([9188, 30, 9]), y_test shape: torch.Size([9188])\n",
            "\n",
            "Each set details:\n",
            "- Training set: 27562 sequences, 30 time steps, 9 features per step.\n",
            "- Validation set: 9187 sequences, 30 time steps, 9 features per step.\n",
            "- Test set: 9188 sequences, 30 time steps, 9 features per step.\n",
            "Number of classes: 3\n"
          ]
        }
      ]
    }
  ]
}