{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwFnJsE6vjf8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from scipy import io #for loadmat, matlab conversion\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt # for plotting training curves\n",
        "from tensorflow import keras #added to save model\n",
        "from tensorflow.keras import layers #format matches MNIST example\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "#imports for computing and displaying output metrics\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
        "import urllib.request # to get files from web w/o !wget\n",
        "import gc\n",
        "\n",
        "# temp - needed for SHL split\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def namestr(obj, namespace):\n",
        "    return [name for name in namespace if namespace[name] is obj]\n",
        "\n",
        "def get_shapes(np_arr_list):\n",
        "    \"\"\"Returns text, each line is shape and dtype for numpy array in list\n",
        "       example: print(get_shapes([X_train, X_test, y_train, y_test]))\"\"\"\n",
        "    shapes = \"\"\n",
        "    for i in np_arr_list:\n",
        "        my_name = namestr(i,globals())\n",
        "        shapes += (my_name[0] + \" shape is \" + str(i.shape) \\\n",
        "            + \" data type is \" + str(i.dtype) + \"\\n\")\n",
        "    return shapes"
      ],
      "metadata": {
        "id": "AB2NWbn1t5NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_py_file(fname, url):\n",
        "    \"\"\"checks for local file, if none downloads from URL.\n",
        "    :return: nothing\"\"\"\n",
        "    if (os.path.exists(fname)):\n",
        "        print (\"Local\",fname, \"found, skipping download\")\n",
        "    else:\n",
        "        print(\"Downloading\",fname, \"from IMICS git repo\")\n",
        "        urllib.request.urlretrieve(url, filename=fname)\n",
        "\n",
        "get_py_file(fname = 'load_data_utils.py', url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_utils.py')\n",
        "\n"
      ],
      "metadata": {
        "id": "p8d0Il_Nt-S4",
        "outputId": "9249593f-9299-4f87-9b38-9a1e8b28f590",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading load_data_utils.py from IMICS git repo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import load_data_utils as utils # ldu just seemed confusing!\n",
        "print('My env_info: \\n' + utils.get_env_info()) # using + vs , gets rid of space"
      ],
      "metadata": {
        "id": "RlMbHJsiuAp9",
        "outputId": "70f2c7bb-c1c0-4971-aecf-1b20ff3eb33b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My env_info: \n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "GPU: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(dataset):\n",
        "    if (dataset == 'MobiAct HAR'):\n",
        "        if (os.path.exists('MobiAct_Dataset_v1.0.zip')):\n",
        "            print (\"Local MobiAct zip found, skipping download\")\n",
        "        else:\n",
        "            !gdown \"1kt9wtIt7N7SIFQAbav7zcZ_PqTa5HegA&confirm=t\" # MobiAct alternate file source\n",
        "        # original share is https://drive.google.com/uc?id=0B5VcW5yHhWhibWxGRTZDd0dGY2s'\n",
        "        # please see https://bmi.hmu.gr/the-mobifall-and-mobiact-datasets-2/ if not working\n",
        "        get_py_file(fname = 'mobiact_adl_load_dataset.py',\n",
        "                url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/HAR/MobiAct/mobiact_adl_load_dataset.py')\n",
        "        from mobiact_adl_load_dataset import mobiact_adl_load_dataset\n",
        "        x_train, y_train, x_valid, y_valid, x_test, y_test = mobiact_adl_load_dataset(incl_val_group = True)\n",
        "        k_size = 50\n",
        "        EPOCHS = 50\n",
        "        t_names = ['JOG','JUM','STD','STN','STU','WAL']\n",
        "    elif (dataset == 'UniMiB SHAR'):\n",
        "        get_py_file(fname = 'unimib_adl_load_dataset.py',\n",
        "                url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/HAR/UniMiB_SHAR/unimib_shar_adl_load_dataset.py')\n",
        "        from unimib_adl_load_dataset import unimib_load_dataset\n",
        "        x_train, y_train, x_valid, y_valid, x_test, y_test = unimib_load_dataset(incl_val_group = True)\n",
        "        t_names = ['StandingUpFS','StandingUpFL','Walking','Running','GoingUpS','Jumping','GoingDownS','LyingDownFS','SittingDown']\n",
        "        k_size = 50\n",
        "        EPOCHS = 60\n",
        "    elif (dataset == 'UCI HAR'):\n",
        "        get_py_file(fname = 'uci_har_load_dataset.py',\n",
        "                    url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/HAR/UCI_HAR/uci_har_load_dataset.py')\n",
        "        from uci_har_load_dataset import uci_har_load_dataset\n",
        "        x_train, y_train, x_valid, y_valid, x_test, y_test = uci_har_load_dataset(incl_val_group = True, incl_xyz_accel= True)\n",
        "        t_names = ['WALKING','WALKING_UPSTAIRS','WALKING_DOWNSTAIRS','SITTING','STANDING','LAYING']\n",
        "        k_size = 50\n",
        "        EPOCHS = 120\n",
        "    elif (dataset == 'TWristAR'):\n",
        "        # Note TWristAR is more updated than the previous datasets so the accesses\n",
        "        # and defaults are a bit different, e.g. t_names is pulled from the .py\n",
        "        get_py_file(fname = 'twristar_load_dataset.py',\n",
        "                    url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/HAR/TWristAR/twristar_load_dataset.py')\n",
        "        import twristar_load_dataset as twristar_load_dataset # diff to get label map\n",
        "        x_train, y_train, x_valid, y_valid, x_test, y_test \\\n",
        "                                = twristar_load_dataset.twristar_load_dataset(\n",
        "                                    incl_val_group = True,\n",
        "                                    one_hot_encode = True)\n",
        "        t_names = list(twristar_load_dataset.label_map_twristar.get('label').keys())\n",
        "        t_names.remove('Undefined')\n",
        "        k_size = 16\n",
        "        EPOCHS = 100\n",
        "    elif (dataset == 'Leotta_2021'):\n",
        "        get_py_file(fname = 'leotta_2021_load_dataset.py',\n",
        "                    url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/ADL/Leotta_2021/leotta_2021_load_dataset.py')\n",
        "        #full_filename = my_path+os.path.join('/ADL/Leotta_2021/'+'leotta_2021_load_dataset.py')\n",
        "        #shutil.copy(full_filename,'leotta_2021_load_dataset.py')\n",
        "\n",
        "        import leotta_2021_load_dataset as leotta_2021_load_dataset\n",
        "        x_train, y_train, x_valid, y_valid, x_test, y_test = leotta_2021_load_dataset.leotta_2021_load_dataset(incl_val_group = True, one_hot_encode = True)\n",
        "        t_names = list(leotta_2021_load_dataset.label_map_leotta.get('label').keys())\n",
        "        k_size = 100\n",
        "        EPOCHS = 50\n",
        "    elif (dataset == 'SHL'):\n",
        "        # SHL takes about 30 minutes to process due to size, using saved arrays for now\n",
        "        !gdown \"1ApHVQ-P2reO6ctNVxeHHxCHeoNlp6c9P&confirm=t\" # SHL 20Hz\n",
        "        utils.unzip_into_dir('SHL_20hz_for_gentry.zip','SHL')\n",
        "        input_dir = './SHL'\n",
        "        x_train = np.load(input_dir + '/'+'x_train.npy')\n",
        "        x_train = np.delete(x_train, [0,1,2], 2) # delete component accel\n",
        "        x_test = np.load(input_dir + '/'+'x_test.npy')\n",
        "        x_test = np.delete(x_test, [0,1,2], 2) # delete component accel\n",
        "        y_train = np.load(input_dir + '/'+'y_train.npy')\n",
        "        y_test = np.load(input_dir + '/'+'y_test.npy')\n",
        "        # also don't have validate working in SHL so just using stratify\n",
        "        x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.10, random_state=42, stratify=y_train)\n",
        "\n",
        "        t_names = ['Still', 'Walking', 'Run', 'Bike', 'Car', 'Bus', 'Train', 'Subway']\n",
        "        k_size = 15\n",
        "        EPOCHS = 100\n",
        "    elif (dataset == 'Gesture Phase Segmentation'):\n",
        "        get_py_file(fname = 'gesture_phase_segmentation_load_dataset.py',\n",
        "                    url = 'https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/Gesturing_Signing/gesture_phase_segmentation_load_dataset.py')\n",
        "        from gesture_phase_segmentation_load_dataset import gesture_phase_segmentation_load_dataset\n",
        "        x_train, y_train, x_valid, y_valid, x_test, y_test, log_info \\\n",
        "                                = gesture_phase_segmentation_load_dataset(\n",
        "                                    incl_val_group = True,\n",
        "                                    return_info_dict = True)\n",
        "        print(\"Note: Due to the size of the Gesture Phase Segmentation and for\",\n",
        "            \"\\ncompatibility, the test arrays are copies of the valid arrays\")\n",
        "        x_test = x_valid.copy()\n",
        "        y_test = y_valid.copy()\n",
        "        t_names = [\"Rest\", \"Preparation\", \"Stroke\",\"Hold\", \"Retraction\"]\n",
        "        k_size = 9\n",
        "        EPOCHS = 100\n",
        "    else:\n",
        "        print('ERROR: dataset unknown')\n",
        "    print(utils.tabulate_numpy_arrays({'x_train':x_train,'y_train':y_train,\n",
        "                                    'x_valid':x_valid,'y_valid':y_valid,\n",
        "                                    'x_test':x_test,'y_test':y_test}))\n",
        "    return x_train, y_train, x_valid, y_valid, x_test, y_test, k_size, EPOCHS, t_names\n"
      ],
      "metadata": {
        "id": "e6b8lmrxuBeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_list = [\n",
        "           \"UniMiB SHAR\",\n",
        "           \"UCI HAR\",\n",
        "           \"TWristAR\",\n",
        "           \"Leotta_2021\",\n",
        "           \"Gesture Phase Segmentation\"\n",
        "           ]\n",
        "pre_validate = True\n",
        "if pre_validate:\n",
        "    for i in ds_list:\n",
        "        dataset = i\n",
        "        print(\"**** Processing \", dataset, \" ****\")\n",
        "        x_train, y_train, x_valid, y_valid, x_test, y_test, k_size, EPOCHS, t_names = get_dataset(dataset)\n",
        "        print(dataset)\n",
        "        print(utils.tabulate_numpy_arrays({\"x_train\":x_train, \"y_train\":y_train,\n",
        "                                        \"x_valid\":x_valid, \"y_valid\":y_valid,\n",
        "                                        \"x_test\" :x_test,  \"y_test\":y_test}))\n",
        "        print(\"k_size =\",k_size,\"(typically a half-second worth of samples or less)\")\n",
        "        print(\"Class names =\",t_names)\n",
        "\n"
      ],
      "metadata": {
        "id": "7ZDwAhJKub7Y",
        "outputId": "06a97087-6ddf-4345-b697-65e586a999f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**** Processing  UniMiB SHAR  ****\n",
            "Downloading unimib_adl_load_dataset.py from IMICS git repo\n",
            "Downloading UniMiB-SHAR.zip file\n",
            "Raw data     shape        object type              data type\n",
            "-----------  -----------  -----------------------  -----------\n",
            "adl_data:    (7579, 453)  <class 'numpy.ndarray'>  float64\n",
            "adl_labels:  (7579, 3)    <class 'numpy.ndarray'>  uint8\n",
            "adl_names:   (9, 1)       <class 'numpy.ndarray'>  object\n",
            "Reshaped data    shape           object type              data type\n",
            "---------------  --------------  -----------------------  -----------\n",
            "adl_data:        (7579, 151, 1)  <class 'numpy.ndarray'>  float64\n",
            "adl_labels:      (7579, 3)       <class 'numpy.ndarray'>  uint8\n",
            "adl_names:       (9, 1)          <class 'numpy.ndarray'>  object\n",
            "x/y_train shape  (4601, 151, 1) (4601,)\n",
            "x/y_validation shape  (1454, 151, 1) (1454,)\n",
            "x/y_test shape   (1524, 151, 1) (1524,)\n",
            "After one-hot encoding\n",
            "x/y_train shape  (4601, 151, 1) (4601, 9)\n",
            "x/y_validation shape  (1454, 151, 1) (1454, 9)\n",
            "x/y_test shape   (1524, 151, 1) (1524, 9)\n",
            "array    shape           data type\n",
            "-------  --------------  -----------\n",
            "x_train  (4601, 151, 1)  float64\n",
            "y_train  (4601, 9)       float32\n",
            "x_valid  (1454, 151, 1)  float64\n",
            "y_valid  (1454, 9)       float32\n",
            "x_test   (1524, 151, 1)  float64\n",
            "y_test   (1524, 9)       float32\n",
            "UniMiB SHAR\n",
            "array    shape           data type\n",
            "-------  --------------  -----------\n",
            "x_train  (4601, 151, 1)  float64\n",
            "y_train  (4601, 9)       float32\n",
            "x_valid  (1454, 151, 1)  float64\n",
            "y_valid  (1454, 9)       float32\n",
            "x_test   (1524, 151, 1)  float64\n",
            "y_test   (1524, 9)       float32\n",
            "k_size = 50 (typically a half-second worth of samples or less)\n",
            "Class names = ['StandingUpFS', 'StandingUpFL', 'Walking', 'Running', 'GoingUpS', 'Jumping', 'GoingDownS', 'LyingDownFS', 'SittingDown']\n",
            "**** Processing  UCI HAR  ****\n",
            "Downloading uci_har_load_dataset.py from IMICS git repo\n",
            "Downloading UCI_HAR_Dataset.zip file\n",
            "Unzipping UCI_HAR_Dataset.zip file\n",
            "\n",
            "Warning: UCI HAR is already split into train/test\n",
            "The validation group is generated using sklearn stratify on train\n",
            "It is not subject independent - confirm accuracy with test set\n",
            "array    shape           data type\n",
            "-------  --------------  -----------\n",
            "x_train  (5514, 128, 4)  float64\n",
            "y_train  (5514, 6)       float32\n",
            "x_valid  (1838, 128, 4)  float64\n",
            "y_valid  (1838, 6)       float32\n",
            "x_test   (2947, 128, 4)  float64\n",
            "y_test   (2947, 6)       float32\n",
            "UCI HAR\n",
            "array    shape           data type\n",
            "-------  --------------  -----------\n",
            "x_train  (5514, 128, 4)  float64\n",
            "y_train  (5514, 6)       float32\n",
            "x_valid  (1838, 128, 4)  float64\n",
            "y_valid  (1838, 6)       float32\n",
            "x_test   (2947, 128, 4)  float64\n",
            "y_test   (2947, 6)       float32\n",
            "k_size = 50 (typically a half-second worth of samples or less)\n",
            "Class names = ['WALKING', 'WALKING_UPSTAIRS', 'WALKING_DOWNSTAIRS', 'SITTING', 'STANDING', 'LAYING']\n",
            "**** Processing  TWristAR  ****\n",
            "Downloading twristar_load_dataset.py from IMICS git repo\n",
            "Downloading load_data_transforms.py from https://raw.githubusercontent.com/imics-lab/load_data_time_series/main/load_data_transforms.py\n",
            "Downloading TWristAR from Zenodo\n",
            "Unzipping TWristAR file in . directory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Due to limited subjects the validation group is a stratified\n",
            "90/10 split of the training group.  It is not subject independent.\n",
            "array    shape          data type\n",
            "-------  -------------  -----------\n",
            "x_train  (1869, 96, 1)  float32\n",
            "y_train  (1869, 6)      uint8\n",
            "x_valid  (208, 96, 1)   float32\n",
            "y_valid  (208, 6)       uint8\n",
            "x_test   (1091, 96, 1)  float32\n",
            "y_test   (1091, 6)      uint8\n",
            "TWristAR\n",
            "array    shape          data type\n",
            "-------  -------------  -----------\n",
            "x_train  (1869, 96, 1)  float32\n",
            "y_train  (1869, 6)      uint8\n",
            "x_valid  (208, 96, 1)   float32\n",
            "y_valid  (208, 6)       uint8\n",
            "x_test   (1091, 96, 1)  float32\n",
            "y_test   (1091, 6)      uint8\n",
            "k_size = 16 (typically a half-second worth of samples or less)\n",
            "Class names = ['Downstairs', 'Jogging', 'Sitting', 'Standing', 'Upstairs', 'Walking']\n",
            "**** Processing  Leotta_2021  ****\n",
            "Downloading leotta_2021_load_dataset.py from IMICS git repo\n",
            "Unzipping Leotta 2021 dataset into ./dataset\n",
            "Using source file ./ADL_Leotta_2021.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "array    shape           data type\n",
            "-------  --------------  -----------\n",
            "x_train  (2391, 300, 3)  float32\n",
            "y_train  (2391, 18)      uint8\n",
            "x_valid  (1167, 300, 3)  float32\n",
            "y_valid  (1167, 18)      uint8\n",
            "x_test   (1987, 300, 3)  float32\n",
            "y_test   (1987, 18)      uint8\n",
            "Leotta_2021\n",
            "array    shape           data type\n",
            "-------  --------------  -----------\n",
            "x_train  (2391, 300, 3)  float32\n",
            "y_train  (2391, 18)      uint8\n",
            "x_valid  (1167, 300, 3)  float32\n",
            "y_valid  (1167, 18)      uint8\n",
            "x_test   (1987, 300, 3)  float32\n",
            "y_test   (1987, 18)      uint8\n",
            "k_size = 100 (typically a half-second worth of samples or less)\n",
            "Class names = ['OTHER', 'RELAX', 'KEYBOARD_WRITING', 'LAPTOP', 'HANDWRITING', 'HANDWASHING', 'FACEWASHING', 'TEETHBRUSH', 'SWEEPING', 'VACUUMING', 'EATING', 'DUSTING', 'RUBBING', 'DOWNSTAIRS', 'WALKING', 'WALKING_FAST', 'UPSTAIRS_FAST', 'UPSTAIRS']\n",
            "**** Processing  Gesture Phase Segmentation  ****\n",
            "Downloading gesture_phase_segmentation_load_dataset.py from IMICS git repo\n",
            "Local load_data_utils.py found, skipping download\n",
            "Local load_data_transforms.py found, skipping download\n",
            "Downloading Gesture-Phase-Segmentation dataset from UCI ML Repository\n",
            "Unzipping Gesture Phase Segmentation file in ./gesture_phase_dataset directory\n",
            "Train: dict_keys(['a1_raw', 'a2_raw', 'a3_raw', 'b1_raw', 'b3_raw'])\n",
            "Valid: dict_keys([])\n",
            "Test : dict_keys(['c1_raw', 'c3_raw'])\n",
            "Warning: Due to limited subjects the validation group is a stratified\n",
            "90/10 split of the training group.  It is not subject independent.\n",
            "Note: Due to the size of the Gesture Phase Segmentation and for \n",
            "compatibility, the test arrays are copies of the valid arrays\n",
            "array    shape          data type\n",
            "-------  -------------  -----------\n",
            "x_train  (422, 30, 18)  float32\n",
            "y_train  (422, 5)       float64\n",
            "x_valid  (47, 30, 18)   float32\n",
            "y_valid  (47, 5)        float64\n",
            "x_test   (47, 30, 18)   float32\n",
            "y_test   (47, 5)        float64\n",
            "Gesture Phase Segmentation\n",
            "array    shape          data type\n",
            "-------  -------------  -----------\n",
            "x_train  (422, 30, 18)  float32\n",
            "y_train  (422, 5)       float64\n",
            "x_valid  (47, 30, 18)   float32\n",
            "y_valid  (47, 5)        float64\n",
            "x_test   (47, 30, 18)   float32\n",
            "y_test   (47, 5)        float64\n",
            "k_size = 9 (typically a half-second worth of samples or less)\n",
            "Class names = ['Rest', 'Preparation', 'Stroke', 'Hold', 'Retraction']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}