{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrWuVf6nvgIu",
        "outputId": "4966a0c8-806b-4864-c892-c9997bc576ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading SharePriceIncrease from https://timeseriesclassification.com/aeon-toolkit/SharePriceIncrease.zip...\n",
            "Extracting SharePriceIncrease...\n",
            "Dataset SharePriceIncrease extracted to datasets/SharePriceIncrease.\n",
            "Loading ARFF file: datasets/SharePriceIncrease/SharePriceIncrease_TRAIN.arff\n",
            "Loading ARFF file: datasets/SharePriceIncrease/SharePriceIncrease_TEST.arff\n",
            "X_train shape: torch.Size([965, 60, 1]), y_train shape: torch.Size([965])\n",
            "X_valid shape: torch.Size([483, 60, 1]), y_valid shape: torch.Size([483])\n",
            "X_test shape: torch.Size([483, 60, 1]), y_test shape: torch.Size([483])\n",
            "Number of classes: 2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.io import arff\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Directory where datasets will be downloaded and extracted\n",
        "DATA_DIR = 'datasets'\n",
        "\n",
        "# Ensure the dataset directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def download_dataset(dataset_name, url):\n",
        "    \"\"\"\n",
        "    Downloads and extracts a zip file containing the dataset.\n",
        "    \"\"\"\n",
        "    zip_path = os.path.join(DATA_DIR, f\"{dataset_name}.zip\")\n",
        "    extract_path = os.path.join(DATA_DIR, dataset_name)\n",
        "\n",
        "    # Download the dataset\n",
        "    print(f\"Downloading {dataset_name} from {url}...\")\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "    # Extract the zip file\n",
        "    print(f\"Extracting {dataset_name}...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    # Remove the zip file after extraction\n",
        "    os.remove(zip_path)\n",
        "    print(f\"Dataset {dataset_name} extracted to {extract_path}.\")\n",
        "    return extract_path\n",
        "\n",
        "def load_arff_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads ARFF file and converts it to a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    print(f\"Loading ARFF file: {file_path}\")\n",
        "    data, meta = arff.loadarff(file_path)\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "def preprocess_data(train_df, test_df, batch_size=64):\n",
        "    \"\"\"\n",
        "    Preprocesses the data:\n",
        "    - Splits the features and labels.\n",
        "    - Normalizes the features.\n",
        "    - Converts them into PyTorch tensors.\n",
        "    - Creates DataLoaders for training, validation, and testing.\n",
        "    \"\"\"\n",
        "    # Separate features and labels\n",
        "    train_features = train_df.drop(columns=['class'])  # Assuming 'class' is the label column\n",
        "    test_features = test_df.drop(columns=['class'])\n",
        "\n",
        "    # Adjust labels to start from 0\n",
        "    train_labels = train_df['class'].apply(lambda x: int(x) - 1).values\n",
        "    test_labels = test_df['class'].apply(lambda x: int(x) - 1).values\n",
        "\n",
        "    # Normalize features\n",
        "    scaler = StandardScaler()\n",
        "    train_features_normalized = scaler.fit_transform(train_features)\n",
        "    test_features_normalized = scaler.transform(test_features)\n",
        "\n",
        "    # Reshape the features into 3D arrays (samples, time_steps, dimensions)\n",
        "    X_train = train_features_normalized.reshape(-1, 60, 1)  # Adjust time_steps to 60\n",
        "    X_test = test_features_normalized.reshape(-1, 60, 1)\n",
        "\n",
        "    # Split test data into validation and test sets\n",
        "    X_valid, X_test, y_valid, y_test = train_test_split(X_test, test_labels, test_size=0.50, random_state=42)\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(train_labels, dtype=torch.int64)\n",
        "\n",
        "    X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
        "    y_valid = torch.tensor(y_valid, dtype=torch.int64)\n",
        "\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.int64)\n",
        "\n",
        "    # Output dataset shapes\n",
        "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "    print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "    # Return both the DataLoaders and the raw tensors\n",
        "    return train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test\n",
        "\n",
        "# Example usage for downloading, extracting, and preprocessing the SharePriceIncrease dataset\n",
        "if __name__ == \"__main__\":\n",
        "    # URL for the dataset (replace with the actual dataset you want to download)\n",
        "    dataset_name = 'SharePriceIncrease'\n",
        "    dataset_url = 'https://timeseriesclassification.com/aeon-toolkit/SharePriceIncrease.zip'\n",
        "    extract_path = download_dataset(dataset_name, dataset_url)\n",
        "\n",
        "    # Load ARFF data\n",
        "    train_file = os.path.join(extract_path, f'{dataset_name}_TRAIN.arff')\n",
        "    test_file = os.path.join(extract_path, f'{dataset_name}_TEST.arff')\n",
        "\n",
        "    # Load data into Pandas DataFrames\n",
        "    train_df = load_arff_data(train_file)\n",
        "    test_df = load_arff_data(test_file)\n",
        "\n",
        "    # Preprocess the data\n",
        "    train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test = preprocess_data(train_df, test_df)\n",
        "\n",
        "    n_classes = len(torch.unique(y_train))\n",
        "\n",
        "    # Output the number of classes\n",
        "    print(f\"Number of classes: {n_classes}\")\n"
      ]
    }
  ]
}