{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LrrqOs4ybB5",
        "outputId": "c6f21fd2-83ca-4dd3-b5a3-907e1f8d8e54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading SelfRegulationSCP1 from https://timeseriesclassification.com/aeon-toolkit/SelfRegulationSCP1.zip...\n",
            "Extracting SelfRegulationSCP1...\n",
            "Dataset SelfRegulationSCP1 extracted to datasets/SelfRegulationSCP1.\n",
            "Loading ARFF file: datasets/SelfRegulationSCP1/SelfRegulationSCP1Dimension1_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP1/SelfRegulationSCP1Dimension2_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP1/SelfRegulationSCP1Dimension3_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP1/SelfRegulationSCP1Dimension4_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP1/SelfRegulationSCP1Dimension5_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP1/SelfRegulationSCP1Dimension6_TRAIN.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP1/SelfRegulationSCP1Dimension1_TEST.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP1/SelfRegulationSCP1Dimension2_TEST.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP1/SelfRegulationSCP1Dimension3_TEST.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP1/SelfRegulationSCP1Dimension4_TEST.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP1/SelfRegulationSCP1Dimension5_TEST.arff\n",
            "Loading ARFF file: datasets/SelfRegulationSCP1/SelfRegulationSCP1Dimension6_TEST.arff\n",
            "X_train shape: torch.Size([268, 896, 6]), y_train shape: torch.Size([268])\n",
            "X_valid shape: torch.Size([146, 896, 6]), y_valid shape: torch.Size([146])\n",
            "X_test shape: torch.Size([147, 896, 6]), y_test shape: torch.Size([147])\n",
            "Number of classes: 2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.io import arff\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Directory where datasets will be downloaded and extracted\n",
        "DATA_DIR = 'datasets'\n",
        "\n",
        "# Ensure the dataset directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def download_dataset(dataset_name, url):\n",
        "    \"\"\"\n",
        "    Downloads and extracts a zip file containing the dataset.\n",
        "    \"\"\"\n",
        "    zip_path = os.path.join(DATA_DIR, f\"{dataset_name}.zip\")\n",
        "    extract_path = os.path.join(DATA_DIR, dataset_name)\n",
        "\n",
        "    # Download the dataset\n",
        "    print(f\"Downloading {dataset_name} from {url}...\")\n",
        "    urllib.request.urlretrieve(url, zip_path)\n",
        "\n",
        "    # Extract the zip file\n",
        "    print(f\"Extracting {dataset_name}...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "\n",
        "    # Remove the zip file after extraction\n",
        "    os.remove(zip_path)\n",
        "    print(f\"Dataset {dataset_name} extracted to {extract_path}.\")\n",
        "    return extract_path\n",
        "\n",
        "def load_arff_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads ARFF file and converts it to a pandas DataFrame.\n",
        "    \"\"\"\n",
        "    print(f\"Loading ARFF file: {file_path}\")\n",
        "    data, meta = arff.loadarff(file_path)\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "def preprocess_data(train_paths, test_paths, batch_size=64):\n",
        "    \"\"\"\n",
        "    Preprocesses the SelfRegulationSCP1 data:\n",
        "    - Loads and combines multiple dimensions from ARFF files.\n",
        "    - Normalizes the features for each dimension.\n",
        "    - Stacks features from different dimensions.\n",
        "    - Converts them into PyTorch tensors.\n",
        "    - Creates DataLoaders for training, validation, and testing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load all training and test dimensions\n",
        "    train_dfs = [load_arff_data(path) for path in train_paths]\n",
        "    test_dfs = [load_arff_data(path) for path in test_paths]\n",
        "\n",
        "    # Separate features and labels for all dimensions\n",
        "    train_features = [df.drop(columns=['cortical']) for df in train_dfs]\n",
        "    test_features = [df.drop(columns=['cortical']) for df in test_dfs]\n",
        "\n",
        "    # Create a label mapping for the two unique class labels\n",
        "    label_mapping = {\n",
        "        b'negativity': 0,\n",
        "        b'positivity': 1\n",
        "    }\n",
        "\n",
        "    # Apply the label mapping to the training and test sets\n",
        "    train_labels = train_dfs[0]['cortical'].apply(lambda x: label_mapping[x]).values\n",
        "    test_labels = test_dfs[0]['cortical'].apply(lambda x: label_mapping[x]).values\n",
        "\n",
        "    # Normalize the features using StandardScaler for each dimension\n",
        "    scalers = [StandardScaler() for _ in range(6)]  # 6 dimensions\n",
        "    train_features_normalized = [scalers[i].fit_transform(train_features[i]) for i in range(6)]\n",
        "    test_features_normalized = [scalers[i].transform(test_features[i]) for i in range(6)]\n",
        "\n",
        "    # Stack all dimensions along a new axis (multivariate time-series)\n",
        "    X_train = np.stack(train_features_normalized, axis=-1)\n",
        "    X_test_full = np.stack(test_features_normalized, axis=-1)\n",
        "\n",
        "    # Split the test data into validation and test sets\n",
        "    X_valid, X_test, y_valid, y_test = train_test_split(X_test_full, test_labels, test_size=0.50, random_state=42)\n",
        "    y_train = train_labels\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.int64)\n",
        "\n",
        "    X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
        "    y_valid = torch.tensor(y_valid, dtype=torch.int64)\n",
        "\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.int64)\n",
        "\n",
        "    # Output dataset shapes\n",
        "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "    print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "    # Return both the DataLoaders and the raw tensors\n",
        "    return train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test\n",
        "\n",
        "# Example usage for downloading, extracting, and preprocessing the SelfRegulationSCP1 dataset\n",
        "if __name__ == \"__main__\":\n",
        "    # URL for the dataset\n",
        "    dataset_name = 'SelfRegulationSCP1'\n",
        "    dataset_url = 'https://timeseriesclassification.com/aeon-toolkit/SelfRegulationSCP1.zip'\n",
        "\n",
        "    # Download and extract the dataset\n",
        "    extract_path = download_dataset(dataset_name, dataset_url)\n",
        "\n",
        "    # Paths for the ARFF files\n",
        "    train_arff_paths = [\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP1Dimension1_TRAIN.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP1Dimension2_TRAIN.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP1Dimension3_TRAIN.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP1Dimension4_TRAIN.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP1Dimension5_TRAIN.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP1Dimension6_TRAIN.arff')\n",
        "    ]\n",
        "\n",
        "    test_arff_paths = [\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP1Dimension1_TEST.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP1Dimension2_TEST.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP1Dimension3_TEST.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP1Dimension4_TEST.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP1Dimension5_TEST.arff'),\n",
        "        os.path.join(extract_path, 'SelfRegulationSCP1Dimension6_TEST.arff')\n",
        "    ]\n",
        "\n",
        "    # Preprocess the data\n",
        "    train_loader, valid_loader, test_loader, X_train, X_valid, X_test, y_train, y_valid, y_test = preprocess_data(train_arff_paths, test_arff_paths)\n",
        "\n",
        "    n_classes = len(torch.unique(y_train))\n",
        "\n",
        "    # Output the number of classes\n",
        "    print(f\"Number of classes: {n_classes}\")\n"
      ]
    }
  ]
}