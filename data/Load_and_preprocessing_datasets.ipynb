{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ArticularyWordRecognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DujbLQFo-qlv",
        "outputId": "d2bef125-37ec-43f8-e987-f5cffef889a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 25\n",
            "X_train shape: (197, 144, 1), y_train shape: (197,)\n",
            "X_valid shape: (50, 144, 1), y_valid shape: (50,)\n",
            "X_test shape: (28, 144, 1), y_test shape: (28,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.io import arff\n",
        "\n",
        "# Step 1: Load the dataset (replace 'your_arff_file.arff' with the actual path)\n",
        "arff_file_path = '/content/ArticularyWordRecognitionDimension1_TRAIN.arff'  # Update the path as needed\n",
        "\n",
        "# Load ARFF file into a Pandas DataFrame\n",
        "data, meta = arff.loadarff(arff_file_path)\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 2: Convert class labels from byte to float\n",
        "df['classAttribute'] = df['classAttribute'].apply(lambda x: float(x.decode('utf-8')))\n",
        "\n",
        "# Step 3: Separate features and labels\n",
        "features = df.drop(columns=['classAttribute'])\n",
        "labels = df['classAttribute']\n",
        "\n",
        "# Step 4: Normalize the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "features_normalized = scaler.fit_transform(features)\n",
        "\n",
        "# Step 5: Reshape the features into a 3D array (samples, time_steps, features)\n",
        "# Here, we assume 144 time steps and 1 feature per time step\n",
        "time_steps = 144  # Assuming 144 channels, we treat each as a time step\n",
        "num_features = 1  # 1 feature per time step (each channel is treated as a feature)\n",
        "X = features_normalized.reshape(-1, time_steps, num_features)\n",
        "\n",
        "# Step 6: Convert labels to a NumPy array\n",
        "y = labels.values\n",
        "\n",
        "unique_labels = np.unique(y)\n",
        "new_label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "# Apply the mapping to the y array\n",
        "y = np.array([new_label_mapping[label] for label in y])\n",
        "\n",
        "# Step 7: Split the dataset into training and testing sets (90% training, 10% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n",
        "\n",
        "# Step 8: Further split the training data into training and validation sets (80% train, 20% validation)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n",
        "\n",
        "# Step 9: Further split the training data into training and validation sets (80% train, 20% validation)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
        "\n",
        "# Step 10: Check the number of classes\n",
        "n_classes = len(np.unique(y_train))\n",
        "print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "# Output the shapes of the datasets\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yx8gMltwKa6r",
        "outputId": "63b912c9-d5a1-423e-b620-342ed1e8d74d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique class labels before processing: [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
            " 19. 20. 21. 22. 23. 24. 25.]\n"
          ]
        }
      ],
      "source": [
        "# Inspect unique values of the classAttribute before processing\n",
        "print(\"Unique class labels before processing:\", df['classAttribute'].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfBJN512Ki8V",
        "outputId": "8ff2efa1-eced-4aa1-ead7-ae9102692573"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique class labels before processing: [b'1.0' b'2.0' b'3.0' b'4.0' b'5.0' b'6.0' b'7.0' b'8.0' b'9.0' b'10.0'\n",
            " b'11.0' b'12.0' b'13.0' b'14.0' b'15.0' b'16.0' b'17.0' b'18.0' b'19.0'\n",
            " b'20.0' b'21.0' b'22.0' b'23.0' b'24.0' b'25.0']\n",
            "Number of classes: 1\n",
            "X_train shape: (197, 144, 1), y_train shape: (197,)\n",
            "X_valid shape: (50, 144, 1), y_valid shape: (50,)\n",
            "X_test shape: (28, 144, 1), y_test shape: (28,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.io import arff\n",
        "\n",
        "# Step 1: Load the dataset (replace 'your_arff_file.arff' with the actual path)\n",
        "arff_file_path = '/content/ArticularyWordRecognitionDimension1_TRAIN.arff'  # Update the path as needed\n",
        "\n",
        "# Load ARFF file into a Pandas DataFrame\n",
        "data, meta = arff.loadarff(arff_file_path)\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 2: Inspect unique class labels before processing\n",
        "print(\"Unique class labels before processing:\", df['classAttribute'].unique())\n",
        "\n",
        "# Step 3: Convert class labels to binary or categorical (assuming 2 classes)\n",
        "# Here you can adjust based on actual class names or values\n",
        "df['classAttribute'] = df['classAttribute'].apply(lambda x: 0 if x == b'Class1' else 1)\n",
        "\n",
        "# Step 4: Separate features and labels\n",
        "features = df.drop(columns=['classAttribute'])\n",
        "labels = df['classAttribute']\n",
        "\n",
        "# Step 5: Normalize the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "features_normalized = scaler.fit_transform(features)\n",
        "\n",
        "# Step 6: Reshape the features into a 3D array (samples, time_steps, features)\n",
        "time_steps = 144  # Assuming 144 channels, we treat each as a time step\n",
        "num_features = 1  # 1 feature per time step (each channel is treated as a feature)\n",
        "X = features_normalized.reshape(-1, time_steps, num_features)\n",
        "\n",
        "# Step 7: Convert labels to a NumPy array\n",
        "y = labels.values\n",
        "\n",
        "# Step 8: Split the dataset into training and testing sets (90% training, 10% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)\n",
        "\n",
        "# Step 9: Further split the training data into training and validation sets (80% train, 20% validation)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
        "\n",
        "# Step 10: Check the number of classes\n",
        "n_classes = len(np.unique(y_train))\n",
        "print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "# Output the shapes of the datasets\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoCTvZbNBy5P",
        "outputId": "07030341-f33f-42fa-86ee-d1ceeb9a213c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Remapped labels: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24]\n"
          ]
        }
      ],
      "source": [
        "unique_labels = np.unique(y)\n",
        "new_label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "# Apply the mapping to the y array\n",
        "y = np.array([new_label_mapping[label] for label in y])\n",
        "\n",
        "# Verify the remapped labels\n",
        "print(f\"Remapped labels: {np.unique(y)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHL-2SnvB39b",
        "outputId": "97936e6d-6859-4e0f-ef6b-bb44f75deb96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 25\n",
            "X_train shape: torch.Size([197, 144, 1]), y_train shape: (197,)\n",
            "X_valid shape: torch.Size([50, 144, 1]), y_valid shape: (50,)\n",
            "X_test shape: torch.Size([28, 144, 1]), y_test shape: (28,)\n"
          ]
        }
      ],
      "source": [
        "n_classes = len(np.unique(y_train))\n",
        "print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "# Output the shapes of the datasets\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AtrialFibrillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ8f6ypGEiGs",
        "outputId": "b3a8d4db-4bb2-42cc-9f4f-ef77e3d8dc90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (21, 640, 1), y_train shape: (21,)\n",
            "X_valid shape: (6, 640, 1), y_valid shape: (6,)\n",
            "X_test shape: (3, 640, 1), y_test shape: (3,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.io import arff\n",
        "\n",
        "# Data Augmentation Function (Jittering)\n",
        "def jitter(data, sigma=0.01):\n",
        "    \"\"\"Add random noise to the data.\"\"\"\n",
        "    return data + np.random.normal(loc=0.0, scale=sigma, size=data.shape)\n",
        "\n",
        "# Load the ARFF file (AtrialFibrillationDimension1_TRAIN.arff)\n",
        "arff_file_path = '/content/AtrialFibrillationDimension1_TRAIN.arff'\n",
        "\n",
        "# Load ARFF file into a Pandas DataFrame\n",
        "data, meta = arff.loadarff(arff_file_path)\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 1: Convert class labels from byte to integer\n",
        "df['target'] = df['target'].apply(lambda x: 1 if x == b'n' else 0)\n",
        "\n",
        "# Step 2: Separate features and labels\n",
        "features = df.drop(columns=['target'])\n",
        "labels = df['target']\n",
        "\n",
        "# Step 3: Normalize the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "features_normalized = scaler.fit_transform(features)\n",
        "\n",
        "# Step 4: Reshape the features into a 3D array (samples, time_steps, features)\n",
        "time_steps = 640  # Number of time steps based on the dataset shape\n",
        "num_features = 1  # We have 1 feature per time step (each column treated as a feature)\n",
        "X = features_normalized.reshape(-1, time_steps, num_features)\n",
        "\n",
        "# Step 5: Convert labels to a NumPy array\n",
        "y = labels.values\n",
        "\n",
        "# Step 6: Apply jitter to augment the dataset\n",
        "X_augmented = np.concatenate([X, jitter(X)], axis=0)\n",
        "y_augmented = np.concatenate([y, y], axis=0)  # Duplicate labels for augmented data\n",
        "\n",
        "# Step 7: Split the dataset into training, validation, and test sets (optional)\n",
        "# You can split the data further if you want to use a fixed validation set.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_augmented, y_augmented, test_size=0.10, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
        "\n",
        "# Output the shapes of the augmented dataset for inspection\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyGIjk5vFSOD",
        "outputId": "48a4ac7e-532e-4dc0-9a17-2f3e5ed08374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (10, 640, 2), y_train shape: (10,)\n",
            "X_valid shape: (3, 640, 2), y_valid shape: (3,)\n",
            "X_test shape: (2, 640, 2), y_test shape: (2,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.io import arff\n",
        "\n",
        "# Load both dimensions\n",
        "arff_file_path_dim1 = '/content/AtrialFibrillationDimension1_TRAIN.arff'\n",
        "arff_file_path_dim2 = '/content/AtrialFibrillationDimension2_TRAIN.arff'\n",
        "\n",
        "# Load ARFF files into DataFrames\n",
        "data_dim1, meta_dim1 = arff.loadarff(arff_file_path_dim1)\n",
        "df_dim1 = pd.DataFrame(data_dim1)\n",
        "\n",
        "data_dim2, meta_dim2 = arff.loadarff(arff_file_path_dim2)\n",
        "df_dim2 = pd.DataFrame(data_dim2)\n",
        "\n",
        "# Step 1: Separate features and labels for both dimensions\n",
        "features_dim1 = df_dim1.drop(columns=['target'])\n",
        "features_dim2 = df_dim2.drop(columns=['target'])\n",
        "labels = df_dim1['target'].apply(lambda x: 1 if x == b'n' else 0)  # Convert target to integer\n",
        "\n",
        "# Step 2: Normalize the features using StandardScaler for both dimensions\n",
        "scaler = StandardScaler()\n",
        "features_dim1_normalized = scaler.fit_transform(features_dim1)\n",
        "features_dim2_normalized = scaler.fit_transform(features_dim2)\n",
        "\n",
        "# Step 3: Stack both dimensions along a new axis (multivariate time-series)\n",
        "X_multivariate = np.stack([features_dim1_normalized, features_dim2_normalized], axis=-1)\n",
        "\n",
        "# Step 4: Convert labels to a NumPy array\n",
        "y = labels.values\n",
        "\n",
        "# Step 5: Split the dataset into training and testing sets (90% training, 10% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_multivariate, y, test_size=0.10, random_state=42)\n",
        "\n",
        "# Step 6: Further split the training data into training and validation sets (80% train, 20% validation)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
        "\n",
        "# Output the shapes of the datasets for inspection\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39eoR8XdH73m",
        "outputId": "e114cb8d-9fbc-4556-bb71-ca1edbd99576"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (12, 640, 2), y_train shape: (12,)\n",
            "X_valid shape: (3, 640, 2), y_valid shape: (3,)\n",
            "X_test shape: (15, 640, 2), y_test shape: (15,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.io import arff\n",
        "\n",
        "# Load both dimensions for training set\n",
        "train_file_path_dim1 = '/content/AtrialFibrillationDimension1_TRAIN.arff'\n",
        "train_file_path_dim2 = '/content/AtrialFibrillationDimension2_TRAIN.arff'\n",
        "\n",
        "# Load ARFF files into DataFrames for training\n",
        "train_data_dim1, train_meta_dim1 = arff.loadarff(train_file_path_dim1)\n",
        "train_df_dim1 = pd.DataFrame(train_data_dim1)\n",
        "\n",
        "train_data_dim2, train_meta_dim2 = arff.loadarff(train_file_path_dim2)\n",
        "train_df_dim2 = pd.DataFrame(train_data_dim2)\n",
        "\n",
        "# Load both dimensions for test set\n",
        "test_file_path_dim1 = '/content/AtrialFibrillationDimension1_TEST.arff'\n",
        "test_file_path_dim2 = '/content/AtrialFibrillationDimension2_TEST.arff'\n",
        "\n",
        "test_data_dim1, test_meta_dim1 = arff.loadarff(test_file_path_dim1)\n",
        "test_df_dim1 = pd.DataFrame(test_data_dim1)\n",
        "\n",
        "test_data_dim2, test_meta_dim2 = arff.loadarff(test_file_path_dim2)\n",
        "test_df_dim2 = pd.DataFrame(test_data_dim2)\n",
        "\n",
        "# Step 1: Separate features and labels for both dimensions in training set\n",
        "train_features_dim1 = train_df_dim1.drop(columns=['target'])\n",
        "train_features_dim2 = train_df_dim2.drop(columns=['target'])\n",
        "train_labels = train_df_dim1['target'].apply(lambda x: 1 if x == b'n' else 0)  # Convert target to integer\n",
        "\n",
        "# Step 2: Separate features and labels for both dimensions in test set\n",
        "test_features_dim1 = test_df_dim1.drop(columns=['target'])\n",
        "test_features_dim2 = test_df_dim2.drop(columns=['target'])\n",
        "test_labels = test_df_dim1['target'].apply(lambda x: 1 if x == b'n' else 0)\n",
        "\n",
        "# Step 3: Normalize the features using StandardScaler for both dimensions in train and test sets\n",
        "scaler_dim1 = StandardScaler()\n",
        "scaler_dim2 = StandardScaler()\n",
        "\n",
        "# Standardize each dimension independently\n",
        "train_features_dim1_normalized = scaler_dim1.fit_transform(train_features_dim1)\n",
        "train_features_dim2_normalized = scaler_dim2.fit_transform(train_features_dim2)\n",
        "\n",
        "test_features_dim1_normalized = scaler_dim1.transform(test_features_dim1)  # Use the same scaler for test data\n",
        "test_features_dim2_normalized = scaler_dim2.transform(test_features_dim2)\n",
        "\n",
        "# Step 4: Stack both dimensions along a new axis (multivariate time-series)\n",
        "X_train = np.stack([train_features_dim1_normalized, train_features_dim2_normalized], axis=-1)\n",
        "X_test = np.stack([test_features_dim1_normalized, test_features_dim2_normalized], axis=-1)\n",
        "\n",
        "# Step 5: Convert labels to a NumPy array\n",
        "y_train = train_labels.values\n",
        "y_test = test_labels.values\n",
        "\n",
        "# Step 6: Split the training data into training and validation sets (80% train, 20% validation)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
        "\n",
        "# Output the shapes of the datasets for inspection\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G5jqR8eIzkY",
        "outputId": "4c2fe3d6-6838-42c9-f747-e19194de5c88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 2\n",
            "X_train shape: torch.Size([12, 640, 2]), y_train shape: (12,)\n",
            "X_valid shape: torch.Size([3, 640, 2]), y_valid shape: (3,)\n",
            "X_test shape: torch.Size([15, 640, 2]), y_test shape: (15,)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Calculate the number of unique classes again\n",
        "n_classes_train = len(np.unique(y_train))\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "# Print the number of unique classes and dataset shapes in the desired format\n",
        "print(f\"Number of classes: {n_classes_train}\")\n",
        "print(f\"X_train shape: {X_train_tensor.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_valid shape: {X_valid_tensor.shape}, y_valid shape: {y_valid.shape}\")\n",
        "print(f\"X_test shape: {X_test_tensor.shape}, y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxvBDwvJJa_H",
        "outputId": "15243119-376a-462e-8ae3-e9b465ad9456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique class labels in the training set: [b'n' b's' b't']\n",
            "Unique class labels in the test set: [b'n' b's' b't']\n"
          ]
        }
      ],
      "source": [
        "# Print unique class labels in the training set\n",
        "unique_classes_train = train_df_dim1['target'].unique()\n",
        "print(\"Unique class labels in the training set:\", unique_classes_train)\n",
        "\n",
        "# Print unique class labels in the test set\n",
        "unique_classes_test = test_df_dim1['target'].unique()\n",
        "print(\"Unique class labels in the test set:\", unique_classes_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCMX9qFCJ5AR",
        "outputId": "0dfc5918-791e-47b5-cfa7-0f86ba055850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (12, 640, 2), y_train shape: (12,)\n",
            "X_valid shape: (3, 640, 2), y_valid shape: (3,)\n",
            "X_test shape: (15, 640, 2), y_test shape: (15,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.io import arff\n",
        "\n",
        "# Load both dimensions for training set\n",
        "train_file_path_dim1 = '/content/AtrialFibrillationDimension1_TRAIN.arff'\n",
        "train_file_path_dim2 = '/content/AtrialFibrillationDimension2_TRAIN.arff'\n",
        "\n",
        "# Load ARFF files into DataFrames for training\n",
        "train_data_dim1, train_meta_dim1 = arff.loadarff(train_file_path_dim1)\n",
        "train_df_dim1 = pd.DataFrame(train_data_dim1)\n",
        "\n",
        "train_data_dim2, train_meta_dim2 = arff.loadarff(train_file_path_dim2)\n",
        "train_df_dim2 = pd.DataFrame(train_data_dim2)\n",
        "\n",
        "# Load both dimensions for test set\n",
        "test_file_path_dim1 = '/content/AtrialFibrillationDimension1_TEST.arff'\n",
        "test_file_path_dim2 = '/content/AtrialFibrillationDimension2_TEST.arff'\n",
        "\n",
        "test_data_dim1, test_meta_dim1 = arff.loadarff(test_file_path_dim1)\n",
        "test_df_dim1 = pd.DataFrame(test_data_dim1)\n",
        "\n",
        "test_data_dim2, test_meta_dim2 = arff.loadarff(test_file_path_dim2)\n",
        "test_df_dim2 = pd.DataFrame(test_data_dim2)\n",
        "\n",
        "# Step 1: Separate features and labels for both dimensions in training set\n",
        "train_features_dim1 = train_df_dim1.drop(columns=['target'])\n",
        "train_features_dim2 = train_df_dim2.drop(columns=['target'])\n",
        "\n",
        "# Updated label mapping\n",
        "train_labels = train_df_dim1['target'].apply(lambda x: 0 if x == b'n' else (1 if x == b's' else 2))\n",
        "\n",
        "# Step 2: Separate features and labels for both dimensions in test set\n",
        "test_features_dim1 = test_df_dim1.drop(columns=['target'])\n",
        "test_features_dim2 = test_df_dim2.drop(columns=['target'])\n",
        "\n",
        "# Updated label mapping for test set\n",
        "test_labels = test_df_dim1['target'].apply(lambda x: 0 if x == b'n' else (1 if x == b's' else 2))\n",
        "\n",
        "# Step 3: Normalize the features using StandardScaler for both dimensions in train and test sets\n",
        "scaler_dim1 = StandardScaler()\n",
        "scaler_dim2 = StandardScaler()\n",
        "\n",
        "# Standardize each dimension independently\n",
        "train_features_dim1_normalized = scaler_dim1.fit_transform(train_features_dim1)\n",
        "train_features_dim2_normalized = scaler_dim2.fit_transform(train_features_dim2)\n",
        "\n",
        "test_features_dim1_normalized = scaler_dim1.transform(test_features_dim1)  # Use the same scaler for test data\n",
        "test_features_dim2_normalized = scaler_dim2.transform(test_features_dim2)\n",
        "\n",
        "# Step 4: Stack both dimensions along a new axis (multivariate time-series)\n",
        "X_train = np.stack([train_features_dim1_normalized, train_features_dim2_normalized], axis=-1)\n",
        "X_test = np.stack([test_features_dim1_normalized, test_features_dim2_normalized], axis=-1)\n",
        "\n",
        "# Step 5: Convert labels to a NumPy array\n",
        "y_train = train_labels.values\n",
        "y_test = test_labels.values\n",
        "\n",
        "# Step 6: Split the training data into training and validation sets (80% train, 20% validation)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
        "\n",
        "# Output the shapes of the datasets for inspection\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCY68wF7KDIz",
        "outputId": "75f6759a-07e4-484c-cebf-579bc906a385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 3\n",
            "X_train shape: torch.Size([12, 640, 2]), y_train shape: (12,)\n",
            "X_valid shape: torch.Size([3, 640, 2]), y_valid shape: (3,)\n",
            "X_test shape: torch.Size([15, 640, 2]), y_test shape: (15,)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Calculate the number of unique classes again\n",
        "n_classes_train = len(np.unique(y_train))\n",
        "\n",
        "# Convert the data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_valid_tensor = torch.tensor(X_valid, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "\n",
        "# Print the number of unique classes and dataset shapes in the desired format\n",
        "print(f\"Number of classes: {n_classes_train}\")\n",
        "print(f\"X_train shape: {X_train_tensor.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_valid shape: {X_valid_tensor.shape}, y_valid shape: {y_valid.shape}\")\n",
        "print(f\"X_test shape: {X_test_tensor.shape}, y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RacketSports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRvjcPQscwfW",
        "outputId": "8ff98a53-3e31-4d56-d78b-4aa469fd461e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 4\n",
            "X_train shape: (151, 30, 6), y_train shape: (151,)\n",
            "X_valid shape: (60, 30, 6), y_valid shape: (60,)\n",
            "X_test shape: (92, 30, 6), y_test shape: (92,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.io import arff\n",
        "\n",
        "# Define the paths for the ARFF files\n",
        "train_arff_paths = [\n",
        "    '/content/RacketSportsDimension1_TRAIN.arff',\n",
        "    '/content/RacketSportsDimension2_TRAIN.arff',\n",
        "    '/content/RacketSportsDimension3_TRAIN.arff',\n",
        "    '/content/RacketSportsDimension4_TRAIN.arff',\n",
        "    '/content/RacketSportsDimension5_TRAIN.arff',\n",
        "    '/content/RacketSportsDimension6_TRAIN.arff'\n",
        "]\n",
        "\n",
        "test_arff_paths = [\n",
        "    '/content/RacketSportsDimension1_TEST.arff',\n",
        "    '/content/RacketSportsDimension2_TEST.arff',\n",
        "    '/content/RacketSportsDimension3_TEST.arff',\n",
        "    '/content/RacketSportsDimension4_TEST.arff',\n",
        "    '/content/RacketSportsDimension5_TEST.arff',\n",
        "    '/content/RacketSportsDimension6_TEST.arff'\n",
        "]\n",
        "\n",
        "# Step 1: Load all training and test dimensions\n",
        "train_dfs = [pd.DataFrame(arff.loadarff(path)[0]) for path in train_arff_paths]\n",
        "test_dfs = [pd.DataFrame(arff.loadarff(path)[0]) for path in test_arff_paths]\n",
        "\n",
        "# Step 2: Separate features and labels for all dimensions\n",
        "train_features = [df.drop(columns=['activity']) for df in train_dfs]\n",
        "test_features = [df.drop(columns=['activity']) for df in test_dfs]\n",
        "\n",
        "# Step 3: Create a label mapping for the four unique class labels\n",
        "label_mapping = {\n",
        "    b'Badminton_Smash': 0,\n",
        "    b'Badminton_Clear': 1,\n",
        "    b'Squash_ForehandBoast': 2,\n",
        "    b'Squash_BackhandBoast': 3\n",
        "}\n",
        "\n",
        "# Apply the label mapping to the training and test sets\n",
        "train_labels = train_dfs[0]['activity'].apply(lambda x: label_mapping[x]).values\n",
        "test_labels = test_dfs[0]['activity'].apply(lambda x: label_mapping[x]).values\n",
        "\n",
        "# Step 4: Normalize the features using StandardScaler for each dimension\n",
        "scalers = [StandardScaler() for _ in range(6)]  # 6 dimensions\n",
        "\n",
        "train_features_normalized = [scalers[i].fit_transform(train_features[i]) for i in range(6)]\n",
        "test_features_normalized = [scalers[i].transform(test_features[i]) for i in range(6)]\n",
        "\n",
        "# Step 5: Stack all dimensions along a new axis (multivariate time-series)\n",
        "X_train = np.stack(train_features_normalized, axis=-1)\n",
        "X_test_full = np.stack(test_features_normalized, axis=-1)\n",
        "\n",
        "# Step 6: Split the test data into validation and test sets\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_test_full, test_labels, test_size=0.60, random_state=42)\n",
        "\n",
        "y_train = train_labels\n",
        "\n",
        "# Step 7: Calculate and print the number of unique classes\n",
        "n_classes = len(np.unique(train_labels))\n",
        "\n",
        "# Output the number of classes and the shapes of the datasets\n",
        "print(f\"Number of classes: {n_classes}\")\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {train_labels.shape}\")\n",
        "print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SelfRegulationSCP1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U59k5My6hR8i",
        "outputId": "bb1d24a7-e837-4301-cd6f-200412b0e5dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 2\n",
            "X_train shape: (268, 896, 6), y_train shape: (268,)\n",
            "X_valid shape: (117, 896, 6), y_valid shape: (117,)\n",
            "X_test shape: (176, 896, 6), y_test shape: (176,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.io import arff\n",
        "\n",
        "# Define the paths for the ARFF files\n",
        "train_arff_paths = [\n",
        "    '/content/SelfRegulationSCP1Dimension1_TRAIN.arff',\n",
        "    '/content/SelfRegulationSCP1Dimension2_TRAIN.arff',\n",
        "    '/content/SelfRegulationSCP1Dimension3_TRAIN.arff',\n",
        "    '/content/SelfRegulationSCP1Dimension4_TRAIN.arff',\n",
        "    '/content/SelfRegulationSCP1Dimension5_TRAIN.arff',\n",
        "    '/content/SelfRegulationSCP1Dimension6_TRAIN.arff'\n",
        "]\n",
        "\n",
        "test_arff_paths = [\n",
        "    '/content/SelfRegulationSCP1Dimension1_TEST.arff',\n",
        "    '/content/SelfRegulationSCP1Dimension2_TEST.arff',\n",
        "    '/content/SelfRegulationSCP1Dimension3_TEST.arff',\n",
        "    '/content/SelfRegulationSCP1Dimension4_TEST.arff',\n",
        "    '/content/SelfRegulationSCP1Dimension5_TEST.arff',\n",
        "    '/content/SelfRegulationSCP1Dimension6_TEST.arff'\n",
        "]\n",
        "\n",
        "# Step 1: Load all training and test dimensions\n",
        "train_dfs = [pd.DataFrame(arff.loadarff(path)[0]) for path in train_arff_paths]\n",
        "test_dfs = [pd.DataFrame(arff.loadarff(path)[0]) for path in test_arff_paths]\n",
        "\n",
        "# Step 2: Separate features and labels for all dimensions\n",
        "train_features = [df.drop(columns=['cortical']) for df in train_dfs]\n",
        "test_features = [df.drop(columns=['cortical']) for df in test_dfs]\n",
        "\n",
        "# Step 3: Create a label mapping for the four unique class labels\n",
        "label_mapping = {\n",
        "    b'negativity': 0,\n",
        "    b'positivity': 1\n",
        "}\n",
        "\n",
        "# Apply the label mapping to the training and test sets\n",
        "train_labels = train_dfs[0]['cortical'].apply(lambda x: label_mapping[x]).values\n",
        "test_labels = test_dfs[0]['cortical'].apply(lambda x: label_mapping[x]).values\n",
        "\n",
        "# Step 4: Normalize the features using StandardScaler for each dimension\n",
        "scalers = [StandardScaler() for _ in range(6)]  # 6 dimensions\n",
        "\n",
        "train_features_normalized = [scalers[i].fit_transform(train_features[i]) for i in range(6)]\n",
        "test_features_normalized = [scalers[i].transform(test_features[i]) for i in range(6)]\n",
        "\n",
        "# Step 5: Stack all dimensions along a new axis (multivariate time-series)\n",
        "X_train = np.stack(train_features_normalized, axis=-1)\n",
        "X_test_full = np.stack(test_features_normalized, axis=-1)\n",
        "\n",
        "# Step 6: Split the test data into validation and test sets\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_test_full, test_labels, test_size=0.60, random_state=42)\n",
        "\n",
        "y_train = train_labels\n",
        "\n",
        "# Step 7: Calculate and print the number of unique classes\n",
        "n_classes = len(np.unique(train_labels))\n",
        "\n",
        "# Output the number of classes and the shapes of the datasets\n",
        "print(f\"Number of classes: {n_classes}\")\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {train_labels.shape}\")\n",
        "print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LSST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FK3g-2yAu-0Z",
        "outputId": "1c334f0f-0d80-4aa3-e061-102865327047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 14\n",
            "X_train shape: (2459, 36, 6), y_train shape: (2459,)\n",
            "X_valid shape: (986, 36, 6), y_valid shape: (986,)\n",
            "X_test shape: (1480, 36, 6), y_test shape: (1480,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.io import arff\n",
        "\n",
        "# Define the paths for the ARFF files\n",
        "train_arff_paths = [\n",
        "    '/content/LSSTDimension1_TRAIN.arff',\n",
        "    '/content/LSSTDimension2_TRAIN.arff',\n",
        "    '/content/LSSTDimension3_TRAIN.arff',\n",
        "    '/content/LSSTDimension4_TRAIN.arff',\n",
        "    '/content/LSSTDimension5_TRAIN.arff',\n",
        "    '/content/LSSTDimension6_TRAIN.arff'\n",
        "]\n",
        "\n",
        "test_arff_paths = [\n",
        "    '/content/LSSTDimension1_TEST.arff',\n",
        "    '/content/LSSTDimension2_TEST.arff',\n",
        "    '/content/LSSTDimension3_TEST.arff',\n",
        "    '/content/LSSTDimension4_TEST.arff',\n",
        "    '/content/LSSTDimension5_TEST.arff',\n",
        "    '/content/LSSTDimension6_TEST.arff'\n",
        "]\n",
        "\n",
        "# Step 1: Load all training and test dimensions\n",
        "train_dfs = [pd.DataFrame(arff.loadarff(path)[0]) for path in train_arff_paths]\n",
        "test_dfs = [pd.DataFrame(arff.loadarff(path)[0]) for path in test_arff_paths]\n",
        "\n",
        "# Step 2: Separate features and labels for all dimensions\n",
        "train_features = [df.drop(columns=['target']) for df in train_dfs]\n",
        "test_features = [df.drop(columns=['target']) for df in test_dfs]\n",
        "\n",
        "# Combine unique labels from both training and test sets\n",
        "unique_labels = np.unique(np.concatenate([train_dfs[0]['target'].unique(), test_dfs[0]['target'].unique()]))\n",
        "\n",
        "# Create a mapping from the byte-encoded labels to integers\n",
        "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "# Apply the mapping to convert byte-encoded labels to integers in both training and test sets\n",
        "train_labels = train_dfs[0]['target'].apply(lambda x: label_mapping[x]).values\n",
        "test_labels = test_dfs[0]['target'].apply(lambda x: label_mapping[x]).values\n",
        "\n",
        "# Apply the label mapping to the training and test sets\n",
        "train_labels = train_dfs[0]['target'].apply(lambda x: label_mapping[x]).values\n",
        "test_labels = test_dfs[0]['target'].apply(lambda x: label_mapping[x]).values\n",
        "\n",
        "# Step 4: Normalize the features using StandardScaler for each dimension\n",
        "scalers = [StandardScaler() for _ in range(6)]  # 6 dimensions\n",
        "\n",
        "train_features_normalized = [scalers[i].fit_transform(train_features[i]) for i in range(6)]\n",
        "test_features_normalized = [scalers[i].transform(test_features[i]) for i in range(6)]\n",
        "\n",
        "# Step 5: Stack all dimensions along a new axis (multivariate time-series)\n",
        "X_train = np.stack(train_features_normalized, axis=-1)\n",
        "X_test_full = np.stack(test_features_normalized, axis=-1)\n",
        "\n",
        "# Step 6: Split the test data into validation and test sets\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_test_full, test_labels, test_size=0.60, random_state=42)\n",
        "\n",
        "y_train = train_labels\n",
        "\n",
        "# Step 7: Calculate and print the number of unique classes\n",
        "n_classes = len(np.unique(train_labels))\n",
        "\n",
        "# Output the number of classes and the shapes of the datasets\n",
        "print(f\"Number of classes: {n_classes}\")\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {train_labels.shape}\")\n",
        "print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGEnoW2QvVaK",
        "outputId": "32446a3b-1582-4192-c14d-c3db260f9cb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['channel_0_0', 'channel_0_1', 'channel_0_2', 'channel_0_3',\n",
            "       'channel_0_4', 'channel_0_5', 'channel_0_6', 'channel_0_7',\n",
            "       'channel_0_8', 'channel_0_9', 'channel_0_10', 'channel_0_11',\n",
            "       'channel_0_12', 'channel_0_13', 'channel_0_14', 'channel_0_15',\n",
            "       'channel_0_16', 'channel_0_17', 'channel_0_18', 'channel_0_19',\n",
            "       'channel_0_20', 'channel_0_21', 'channel_0_22', 'channel_0_23',\n",
            "       'channel_0_24', 'channel_0_25', 'channel_0_26', 'channel_0_27',\n",
            "       'channel_0_28', 'channel_0_29', 'channel_0_30', 'channel_0_31',\n",
            "       'channel_0_32', 'channel_0_33', 'channel_0_34', 'channel_0_35',\n",
            "       'target'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Print the column names of the first dimension's training data to inspect the label column\n",
        "print(train_dfs[0].columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsEDniXcvemq",
        "outputId": "dc5440f0-a41f-4219-833d-886d843dbc4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique class labels in the training set: [b'6' b'15' b'16' b'42' b'52' b'53' b'62' b'64' b'65' b'67' b'88' b'90'\n",
            " b'92' b'95']\n",
            "Unique class labels in the test set: [b'6' b'15' b'16' b'42' b'52' b'53' b'62' b'64' b'65' b'67' b'88' b'90'\n",
            " b'92' b'95']\n"
          ]
        }
      ],
      "source": [
        "# Inspect the unique labels in the 'activity' column\n",
        "print(\"Unique class labels in the training set:\", train_dfs[0]['target'].unique())\n",
        "print(\"Unique class labels in the test set:\", test_dfs[0]['target'].unique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SharePriceIncrease"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTMESfpjgDsv",
        "outputId": "91f9781b-eeed-4b49-ca72-c6b657323235"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 2\n",
            "X_train shape: torch.Size([965, 60, 1]), y_train shape: torch.Size([965])\n",
            "X_valid shape: torch.Size([386, 60, 1]), y_valid shape: torch.Size([386])\n",
            "X_test shape: torch.Size([579, 60, 1]), y_test shape: torch.Size([579])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.io import arff\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Paths for the ARFF files (update with your local paths)\n",
        "train_arff_path = '/content/SharePriceIncrease_TRAIN.arff'\n",
        "test_arff_path = '/content/SharePriceIncrease_TEST.arff'\n",
        "\n",
        "# Step 1: Load the ARFF files for training and test sets\n",
        "train_data, train_meta = arff.loadarff(train_arff_path)\n",
        "test_data, test_meta = arff.loadarff(test_arff_path)\n",
        "\n",
        "# Convert to Pandas DataFrames\n",
        "train_df = pd.DataFrame(train_data)\n",
        "test_df = pd.DataFrame(test_data)\n",
        "\n",
        "# Step 2: If the training set has an extra row, we can restore it to 965 samples\n",
        "train_df = train_df.iloc[:965]  # Ensure 965 samples\n",
        "test_df = test_df.iloc[:965]  # Ensure 965 samples\n",
        "\n",
        "# Step 3: Separate features and labels for both training and test sets\n",
        "train_features = train_df.drop(columns=['class'])  # Assuming 'class' is the label column\n",
        "test_features = test_df.drop(columns=['class'])\n",
        "\n",
        "# Use the labels from the 'class' column\n",
        "train_labels = train_df['class'].apply(lambda x: int(x)).values  # Assuming labels are binary\n",
        "test_labels = test_df['class'].apply(lambda x: int(x)).values\n",
        "\n",
        "# Step 4: Normalize the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "train_features_normalized = scaler.fit_transform(train_features)\n",
        "test_features_normalized = scaler.transform(test_features)\n",
        "\n",
        "# Step 5: Reshape the features into 3D arrays (samples, time_steps, dimensions)\n",
        "X_train = train_features_normalized.reshape(-1, 60, 1)  # 60 time steps, 1 dimension\n",
        "X_test = test_features_normalized.reshape(-1, 60, 1)\n",
        "\n",
        "# Step 6: Split the test data into validation and test sets, ensuring equal sizes\n",
        "# Set test_size=0.5 to ensure a 50-50 split\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_test, test_labels, test_size=0.60, random_state=42)\n",
        "\n",
        "y_train = train_labels\n",
        "\n",
        "# Step 7: Convert data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.int64)\n",
        "\n",
        "X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
        "y_valid = torch.tensor(y_valid, dtype=torch.int64)\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.int64)\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "valid_dataset = TensorDataset(X_valid, y_valid)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "# Step 8: Calculate and print the number of unique classes\n",
        "n_classes = len(np.unique(train_labels))\n",
        "\n",
        "# Output the number of classes and the shapes of the datasets\n",
        "print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "# Output dataset shapes\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ20aE5sgcZ4",
        "outputId": "c49a4025-a31e-46c9-de4f-6aa7690743f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['t-60', 't-59', 't-58', 't-57', 't-56', 't-55', 't-54', 't-53', 't-52',\n",
            "       't-51', 't-50', 't-49', 't-48', 't-47', 't-46', 't-45', 't-44', 't-43',\n",
            "       't-42', 't-41', 't-40', 't-39', 't-38', 't-37', 't-36', 't-35', 't-34',\n",
            "       't-33', 't-32', 't-31', 't-30', 't-29', 't-28', 't-27', 't-26', 't-25',\n",
            "       't-24', 't-23', 't-22', 't-21', 't-20', 't-19', 't-18', 't-17', 't-16',\n",
            "       't-15', 't-14', 't-13', 't-12', 't-11', 't-10', 't-9', 't-8', 't-7',\n",
            "       't-6', 't-5', 't-4', 't-3', 't-2', 't-1', 'class'],\n",
            "      dtype='object')\n",
            "Index(['t-60', 't-59', 't-58', 't-57', 't-56', 't-55', 't-54', 't-53', 't-52',\n",
            "       't-51', 't-50', 't-49', 't-48', 't-47', 't-46', 't-45', 't-44', 't-43',\n",
            "       't-42', 't-41', 't-40', 't-39', 't-38', 't-37', 't-36', 't-35', 't-34',\n",
            "       't-33', 't-32', 't-31', 't-30', 't-29', 't-28', 't-27', 't-26', 't-25',\n",
            "       't-24', 't-23', 't-22', 't-21', 't-20', 't-19', 't-18', 't-17', 't-16',\n",
            "       't-15', 't-14', 't-13', 't-12', 't-11', 't-10', 't-9', 't-8', 't-7',\n",
            "       't-6', 't-5', 't-4', 't-3', 't-2', 't-1', 'class'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Print the columns of the DataFrame to find the correct label column\n",
        "print(train_df.columns)\n",
        "print(test_df.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9Jlo2LFhDyE",
        "outputId": "21b502a4-8f52-417d-f73b-e0c8b32efb4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rows in the training set: 966\n",
            "Number of rows in the test set: 966\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of rows in the training set: {len(train_df)}\")\n",
        "print(f\"Number of rows in the test set: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAkENhlWhHQy",
        "outputId": "51e4953f-a3ce-4535-a253-32bf31ee9c7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicates in the training set: 0\n",
            "Number of duplicates in the test set: 0\n"
          ]
        }
      ],
      "source": [
        "# Check for duplicate rows in the DataFrames\n",
        "train_duplicates = train_df[train_df.duplicated()]\n",
        "test_duplicates = test_df[test_df.duplicated()]\n",
        "\n",
        "print(f\"Number of duplicates in the training set: {len(train_duplicates)}\")\n",
        "print(f\"Number of duplicates in the test set: {len(test_duplicates)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MelbournePedestrian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ00ArCOFFfY",
        "outputId": "9085cc33-98f7-4fe8-c253-e122a34ae883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 10\n",
            "X_train shape: torch.Size([1138, 24, 1]), y_train shape: torch.Size([1138])\n",
            "X_valid shape: torch.Size([1159, 24, 1]), y_valid shape: torch.Size([1159])\n",
            "X_test shape: torch.Size([1160, 24, 1]), y_test shape: torch.Size([1160])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.io import arff\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Paths for the ARFF files (update with your local paths)\n",
        "train_arff_path = '/content/MelbournePedestrian_nmv_TRAIN.arff'\n",
        "test_arff_path = '/content/MelbournePedestrian_nmv_TEST.arff'\n",
        "\n",
        "# Step 1: Load the ARFF files for training and test sets\n",
        "train_data, train_meta = arff.loadarff(train_arff_path)\n",
        "test_data, test_meta = arff.loadarff(test_arff_path)\n",
        "\n",
        "# Convert to Pandas DataFrames\n",
        "train_df = pd.DataFrame(train_data)\n",
        "test_df = pd.DataFrame(test_data)\n",
        "\n",
        "# Step 2: Separate features and labels for both training and test sets\n",
        "train_features = train_df.drop(columns=['target'])  # Assuming 'class' is the label column\n",
        "test_features = test_df.drop(columns=['target'])\n",
        "\n",
        "# Use the labels from the 'class' column\n",
        "train_labels = train_df['target'].apply(lambda x: int(x)).values  # Assuming labels are numeric\n",
        "test_labels = test_df['target'].apply(lambda x: int(x)).values\n",
        "\n",
        "# Step 3: Normalize the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "train_features_normalized = scaler.fit_transform(train_features)\n",
        "test_features_normalized = scaler.transform(test_features)\n",
        "\n",
        "# Step 4: Reshape the features into 3D arrays (samples, time_steps, dimensions)\n",
        "# Since this dataset has 24 time steps and 1 dimension\n",
        "X_train = train_features_normalized.reshape(-1, 24, 1)  # 24 time steps, 1 dimension\n",
        "X_test = test_features_normalized.reshape(-1, 24, 1)\n",
        "\n",
        "# Step 5: Split the test data into validation and test sets (optional)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_test, test_labels, test_size=0.50, random_state=42)\n",
        "\n",
        "# Step 6: Convert data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(train_labels, dtype=torch.int64)\n",
        "\n",
        "X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
        "y_valid = torch.tensor(y_valid, dtype=torch.int64)\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.int64)\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "valid_dataset = TensorDataset(X_valid, y_valid)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "# Step 8: Calculate and print the number of unique classes\n",
        "n_classes = len(np.unique(train_labels))\n",
        "\n",
        "# Output the number of classes and the shapes of the datasets\n",
        "print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "# Output dataset shapes\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIsoVmk9Fa7X",
        "outputId": "4b01f26b-df7b-4e7f-9baa-fc2208a49a59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['att1', 'att2', 'att3', 'att4', 'att5', 'att6', 'att7', 'att8', 'att9',\n",
            "       'att10', 'att11', 'att12', 'att13', 'att14', 'att15', 'att16', 'att17',\n",
            "       'att18', 'att19', 'att20', 'att21', 'att22', 'att23', 'att24',\n",
            "       'target'],\n",
            "      dtype='object')\n",
            "Index(['att1', 'att2', 'att3', 'att4', 'att5', 'att6', 'att7', 'att8', 'att9',\n",
            "       'att10', 'att11', 'att12', 'att13', 'att14', 'att15', 'att16', 'att17',\n",
            "       'att18', 'att19', 'att20', 'att21', 'att22', 'att23', 'att24',\n",
            "       'target'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Print the columns of the DataFrame to find the correct label column\n",
        "print(train_df.columns)\n",
        "print(test_df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ElectricDevices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58oFwrxnHlGx",
        "outputId": "43ab65ff-12f2-473f-cd9f-cc63924f6b07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of classes: 7\n",
            "X_train shape: torch.Size([8926, 96, 1]), y_train shape: torch.Size([8926])\n",
            "X_valid shape: torch.Size([3855, 96, 1]), y_valid shape: torch.Size([3855])\n",
            "X_test shape: torch.Size([3856, 96, 1]), y_test shape: torch.Size([3856])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.io import arff\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Paths for the ARFF files (update with your local paths)\n",
        "train_arff_path = '/content/ElectricDevices_TRAIN.arff'\n",
        "test_arff_path = '/content/ElectricDevices_TEST.arff'\n",
        "\n",
        "# Step 1: Load the ARFF files for training and test sets\n",
        "train_data, train_meta = arff.loadarff(train_arff_path)\n",
        "test_data, test_meta = arff.loadarff(test_arff_path)\n",
        "\n",
        "# Convert to Pandas DataFrames\n",
        "train_df = pd.DataFrame(train_data)\n",
        "test_df = pd.DataFrame(test_data)\n",
        "\n",
        "# Step 2: Separate features and labels for both training and test sets\n",
        "train_features = train_df.drop(columns=['target'])  # Assuming 'class' is the label column\n",
        "test_features = test_df.drop(columns=['target'])\n",
        "\n",
        "# Use the labels from the 'class' column\n",
        "train_labels = train_df['target'].apply(lambda x: int(x)).values  # Assuming labels are numeric\n",
        "test_labels = test_df['target'].apply(lambda x: int(x)).values\n",
        "\n",
        "# Step 3: Normalize the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "train_features_normalized = scaler.fit_transform(train_features)\n",
        "test_features_normalized = scaler.transform(test_features)\n",
        "\n",
        "# Step 4: Reshape the features into 3D arrays (samples, time_steps, dimensions)\n",
        "# Since this dataset has 24 time steps and 1 dimension\n",
        "X_train = train_features_normalized.reshape(-1, 96, 1)  # 24 time steps, 1 dimension\n",
        "X_test = test_features_normalized.reshape(-1, 96, 1)\n",
        "\n",
        "# Step 5: Split the test data into validation and test sets (optional)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_test, test_labels, test_size=0.50, random_state=42)\n",
        "\n",
        "# Step 6: Convert data to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(train_labels, dtype=torch.int64)\n",
        "\n",
        "X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
        "y_valid = torch.tensor(y_valid, dtype=torch.int64)\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.int64)\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "valid_dataset = TensorDataset(X_valid, y_valid)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "# Step 8: Calculate and print the number of unique classes\n",
        "n_classes = len(np.unique(train_labels))\n",
        "\n",
        "# Output the number of classes and the shapes of the datasets\n",
        "print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "# Output dataset shapes\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDgqvnPA-0Bb",
        "outputId": "e983e5e8-aa1d-4015-d7a5-a9c41443a45c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "@problemName train.ts\n",
            "@timestamps false\n",
            "@univariate true\n",
            "@equalLength true\n",
            "@seriesLength 178\n",
            "@classLabel true 0 1 2 3 4\n",
            "@data\n",
            "6.0420027,1.8691087,7.432967,-8.997802,-10.041026,-20.212454,-8.128449,-9.95409,-7.085226,-13.431501,4.911844,-6.1289377,-1.4344323,2.303785,-1.0866911,-14.909402,-10.214896,-7.432967,-12.388278,-11.779732,-2.912332,-4.5641026,0.3912088,-3.6078143,4.3032966,5.34652,-4.477167,-7.6068377,1.1736264,7.7807083,9.171673,2.1299145,-2.912332,3.520879,9.519414,-3.520879,-7.172161,-15.691819,-2.7384615,-8.736997,8.389256,5.955067,18.47375,-5.5203905,-1.347497,-8.476191,-3.8686202,-18.734554,-10.388767,-0.3912088,7.693773,9.258608,13.692307,3.8686202,5.694261,-0.043467645,4.042491,0.9128205,-2.303785,8.476191,7.3460317,-6.5636144,8.389256,1.6952381,13.8661785,1.2605617,10.041026,-13.344566,-1.956044,0.3912088,4.911844,0.82588524,11.431991,2.9992673,18.47375,5.0857143,9.345543,10.649572,7.693773,0.30427352,-0.13040294,5.34652,7.8676434,11.692796,25.776312,3.9555554,9.345543,-0.47814408,0.9997558,1.2605617,-5.1726494,-11.692796,-4.477167,-4.3032966,-3.520879,7.6068377,2.8253968,-1.6952381,2.3907204,-0.043467645,-7.085226,-22.820513,-15.083272,-8.041514,-1.6952381,1.956044,-2.303785,5.8681316,2.912332,-4.6510377,-1.2605617,-4.5641026,-5.8681316,1.6952381,-1.1736264,0.6520147,-1.0866911,-8.650061,-5.7811966,12.649084,0.3912088,1.347497,-3.8686202,-3.173138,2.8253968,6.6505494,4.998779,7.7807083,0.6520147,2.8253968,1.7821734,0.21733822,8.30232,4.998779,1.0866911,-5.259585,-3.3470085,-4.911844,5.1726494,-4.911844,-6.5636144,-4.998779,-4.477167,4.129426,10.562637,7.519902,12.127473,5.0857143,17.256655,11.0842495,16.821978,9.78022,5.34652,4.6510377,1.2605617,10.301831,8.389256,10.9973135,7.693773,19.516972,5.1726494,3.2600732,8.997802,15.691819,9.693284,22.559708,14.38779,9.606349,18.64762,5.8681316,12.301343,2.6515262,5.34652,-13.605372,-11.518926,-9.519414,-7.172161,-1.6083028:0\n",
            "-1.6083028,-13.170696,-8.997802,1.347497,5.259585,-1.5213675,2.6515262,4.6510377,3.7816849,-1.5213675,1.1736264,-2.6515262,-0.73894995,5.5203905,2.9992673,15.083272,13.170696,10.562637,-11.345055,1.6952381,-2.9992673,-4.3032966,-7.3460317,-0.6520147,-4.129426,-9.171673,2.6515262,19.777779,7.2590966,13.605372,6.0420027,8.128449,-1.0866911,8.910867,0.6520147,11.25812,2.3907204,6.0420027,10.562637,19.516972,11.345055,17.082785,7.6068377,13.8661785,0.5650794,-11.25812,-6.6505494,-5.7811966,-0.3912088,10.475702,9.345543,-6.2158732,3.3470085,3.520879,-0.30427352,3.520879,7.432967,-7.6068377,-14.38779,-11.866667,0.13040294,3.6947496,2.303785,-1.6952381,4.390232,-2.2168498,0.21733822,11.171185,10.649572,7.2590966,-5.4334555,-17.34359,-14.996337,-0.9997558,1.5213675,7.2590966,0.30427352,2.1299145,8.30232,6.476679,8.30232,6.389744,4.390232,-6.5636144,-15.952625,-4.5641026,-1.4344323,2.3907204,12.822955,18.99536,-0.043467645,5.1726494,12.475214,5.259585,-4.5641026,-8.823932,-12.127473,-35.773872,-27.080341,-6.9982905,7.6068377,9.432479,4.6510377,14.735531,11.0842495,9.258608,11.431991,-6.82442,-1.6083028,-13.344566,-6.2158732,-10.562637,-4.5641026,-8.128449,-3.3470085,-6.2158732,-8.650061,-2.303785,6.82442,9.432479,13.344566,2.564591,-6.1289377,-5.34652,3.7816849,8.563126,4.998779,1.1736264,0.82588524,4.2163615,11.171185,9.867155,15.517949,-4.737973,-3.4339437,-7.519902,4.477167,-3.6078143,1.6952381,-3.6078143,-9.867155,-14.996337,-13.170696,-4.998779,-5.7811966,-12.649084,-1.8691087,-16.474237,-1.6952381,-8.650061,4.8249083,-1.6952381,-1.4344323,-7.2590966,-9.432479,0.043467645,-10.562637,2.3907204,-12.996825,-16.735043,-18.126007,-16.474237,1.7821734,-3.0862026,7.3460317,1.956044,0.9128205,-3.520879,-8.041514,-6.5636144,-4.998779,-5.34652,-6.389744,-11.171185,-2.3907204,1.0866911,6.9113555,6.1289377:0\n",
            "-2.8253968,2.3907204,3.9555554,-4.2163615,2.4776556,2.1299145,3.9555554,-5.1726494,5.694261,1.6083028,-4.2163615,-7.2590966,-0.47814408,8.389256,-5.259585,4.8249083,9.345543,14.648596,15.517949,4.737973,-0.3912088,5.694261,-3.520879,-1.8691087,4.129426,2.912332,-0.5650794,11.0842495,6.476679,10.475702,8.389256,2.2168498,3.7816849,7.6068377,1.5213675,6.0420027,8.2153845,-1.2605617,-2.7384615,-7.432967,7.2590966,2.912332,12.736019,0.9128205,0.3912088,-15.778754,-14.474726,-9.606349,-9.258608,-0.13040294,-12.214408,2.912332,-10.214896,-5.7811966,-9.171673,-19.777779,-8.30232,-4.129426,-9.258608,5.259585,1.6083028,7.7807083,11.779732,5.694261,-3.4339437,7.172161,-1.7821734,-3.520879,-12.475214,-7.6068377,-5.8681316,2.7384615,9.258608,22.211966,23.42906,12.736019,8.389256,9.78022,5.7811966,1.6952381,-7.954579,-8.476191,-6.389744,16.648108,6.5636144,8.563126,1.956044,1.6083028,-12.127473,-9.693284,-8.650061,-6.476679,-11.171185,-7.8676434,-18.99536,2.9992673,0.6520147,-2.564591,-2.3907204,-6.82442,-4.8249083,-6.82442,-8.563126,-12.040537,-8.30232,-6.389744,-1.5213675,-2.6515262,-4.390232,-0.47814408,8.736997,-4.8249083,-7.519902,-0.13040294,-5.8681316,-4.2163615,-3.4339437,-1.2605617,8.041514,5.0857143,-3.9555554,2.912332,2.7384615,6.737485,-3.520879,8.823932,-1.0866911,1.6952381,-1.0866911,-0.9128205,-4.998779,3.8686202,-7.693773,-2.6515262,-9.95409,-4.042491,-6.82442,-8.128449,-4.042491,4.8249083,-5.7811966,5.259585,-5.694261,10.127961,-9.084738,-3.7816849,-3.6078143,-3.6947496,6.476679,3.0862026,4.998779,1.347497,-6.476679,-3.4339437,-10.127961,-7.6068377,-11.779732,-9.78022,-0.5650794,3.9555554,-1.956044,-0.9128205,4.5641026,0.21733822,4.6510377,-5.8681316,-4.6510377,6.82442,4.129426,7.085226,-2.3907204,-2.303785,-3.6947496,4.5641026,-6.2158732,-2.7384615,-4.477167,-4.390232:0\n",
            "-15.952625,-3.3470085,-15.517949,-9.432479,-16.474237,-6.6505494,-14.909402,-12.214408,-10.823443,-7.2590966,-14.474726,-3.4339437,-1.2605617,7.693773,5.694261,0.30427352,-6.82442,-6.0420027,-7.172161,-8.910867,-2.1299145,-0.82588524,-16.735043,4.8249083,-4.3032966,10.214896,8.476191,2.2168498,6.476679,2.564591,2.564591,3.7816849,-2.564591,-7.432967,-4.042491,11.866667,-1.8691087,7.954579,5.4334555,3.173138,1.0866911,1.4344323,2.1299145,5.259585,3.520879,-3.6947496,1.7821734,5.7811966,-0.82588524,-2.9992673,2.9992673,-1.7821734,-1.8691087,-6.5636144,0.6520147,7.693773,4.911844,6.737485,-0.043467645,3.3470085,0.5650794,9.78022,-0.043467645,-4.2163615,-3.9555554,7.519902,8.041514,9.693284,6.2158732,7.954579,5.694261,1.1736264,11.0842495,3.520879,6.737485,9.432479,16.561172,19.777779,21.95116,18.039072,17.082785,13.953114,4.390232,1.7821734,-8.476191,3.6078143,2.9992673,10.475702,14.38779,18.560684,14.735531,11.431991,3.9555554,3.4339437,-6.82442,-3.6078143,0.30427352,10.9973135,13.605372,13.344566,9.171673,-0.6520147,-4.042491,-9.78022,-11.953602,-16.474237,-8.30232,-9.432479,-1.347497,8.041514,7.6068377,10.301831,7.432967,3.6947496,3.6947496,3.6078143,-3.520879,8.650061,0.30427352,7.8676434,-0.9997558,-4.998779,-6.737485,-9.432479,-5.955067,-2.3907204,0.9128205,11.953602,3.520879,13.344566,10.649572,6.82442,13.344566,-2.564591,-6.737485,-8.650061,-3.173138,4.390232,10.736508,16.908913,4.5641026,8.476191,3.4339437,7.172161,9.606349,6.0420027,10.214896,6.476679,15.257143,20.907936,19.343102,16.735043,17.430525,12.90989,16.995848,18.560684,6.82442,3.8686202,7.7807083,3.8686202,-3.6947496,11.345055,-2.6515262,-7.085226,-15.952625,-14.300855,-10.388767,-2.7384615,-4.390232,-0.73894995,4.477167,12.996825,21.95116,21.081806,21.603418,14.909402,9.345543,6.9982905:0\n"
          ]
        }
      ],
      "source": [
        "# Inspect the first few lines of the .ts file\n",
        "with open(train_file_path, 'r') as file:\n",
        "    for i, line in enumerate(file):\n",
        "        print(line.strip())\n",
        "        if i >= 10:  # Limit to the first 10 lines\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sleep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWm74dyg_Nkr",
        "outputId": "bca26a72-830e-452c-b1bd-b73edcb5ab60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping invalid line: 29.18022,30.915506,30.550182,27.262272,17.581196,7.808791,8.996093,13.562637,22.787058,25.435654,27.992918,14.019292,-5.2515264,0.95897436,-3.2422466,7.900122,18.95116,18.768497,16.941881,6.7128205,-3.1509159,-11.644689,-10.000732,-8.813431,0.6849817,1.1416361,3.6989012,10.274725,15.0239315,9.361417,7.3521366,6.6214895,10.183394,11.462027,9.72674,7.808791,10.457387,5.0688643,1.4156288,14.658608,22.695726,28.449574,32.468132,31.006838,25.344322,16.667887,18.403175,18.403175,15.2065935,10.7313795,6.5301585,9.452747,4.3382173,-0.7763126,-10.000732,-18.494505,-26.348963,-32.10281,-38.130646,-40.59658,-44.249817,-45.163124,-42.4232,-50.186325,-50.186325,-52.378265,-56.214165,-56.76215,-59.593407,-59.228085,-51.46496,-55.026863,-54.93553,-50.64298,-54.296215,-49.181686,-48.359707,-49.54701,-45.61978,-44.889133,-45.43712,-44.43248,-45.163124,-47.172405,-44.797802,-39.59194,-33.29011,-32.742123,-26.896948,-22.787058,-24.339682,-24.339682,-21.873749,-10.640049,1.3242979,7.260806,16.576557,15.389256,16.576557,25.161661,18.403175,23.152382,24.522345,27.718925,33.929424,39.957264,38.861294,41.69255,41.235897,39.40928,37.673992,37.21734,36.304028,35.39072,40.870575,45.254456,46.53309,41.783882,40.50525,40.779243,32.01148,32.468132,36.39536,32.833454,37.673992,29.819536,28\n",
            "X_train shape: (17252, 178), y_train shape: (17252,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to load the .ts file\n",
        "def load_ts_file(file_path):\n",
        "    data = []\n",
        "    labels = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        is_metadata = True\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if is_metadata:\n",
        "                if line.lower() == \"@data\":\n",
        "                    is_metadata = False\n",
        "                continue\n",
        "            if len(line) > 0:\n",
        "                # Try to split the line into series and label\n",
        "                try:\n",
        "                    series, label = line.split(':')\n",
        "                    series = np.array(series.split(','), dtype=np.float32)\n",
        "                    data.append(series)\n",
        "                    labels.append(int(label))  # Convert the label to an integer\n",
        "                except ValueError:\n",
        "                    print(f\"Skipping invalid line: {line}\")\n",
        "                    continue  # Skip lines that don't conform to the expected format\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "# Test by loading the file\n",
        "train_file_path = '/content/Sleep_TRAIN.ts'\n",
        "X_train, y_train = load_ts_file(train_file_path)\n",
        "\n",
        "# Print the loaded data\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37wz_RHMExLJ",
        "outputId": "4a389283-af3b-461f-ee73-e92f5e2ebed1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipping invalid line: -20.742857,-25.674725,-20.356045,-10.589011,7.591209,19.485714,34.861538,40.083515,44.62857,41.630768,35.73187,31.863737,33.604397,36.6022,38.43956,38.246155,32.734066,18.808792,9.041759,3.7230768,14.553846,17.261538,9.912087,5.6571426,5.753846,6.720879,-1.3054945,-14.747252,-21.903297,-29.736263,-29.542856,-29.156044,-32.34725,-38.246155,-48.98022,-50.527473,-51.204395,-52.84835,-55.362637,-53.815384,-50.914288,-51.784615,-52.84835,-55.556046,-57.393406,-57.78022,-60.197803,-68.03077,-66.38681,-72.09231,-71.512085,-64.64616,-56.426373,-54.105495,-47.43297,-48.206593,-49.56044,-41.630768,-41.14725,-28.76923,-28.865934,-33.12088,-33.797802,-34.764835,-36.698902,-40.663734,-38.923077,-34.861538,-27.705494,-23.934067,-21.226374,-20.25934,-15.810989,-18.51868,-7.978022,-4.4,10.008791,25.094505,30.413187,33.314285,38.246155,30.703297,31.573626,29.349451,27.512089,22.483517,24.901098,20.452747,21.323076,21.903297,21.323076,28.865934,31.573626,34.184616,39.503296,55.16923,58.553844,59.520878,58.843956,49.850548,39.30989,21.41978,13.490109,14.167033,13.973626,18.808792,21.613186,34.37802,47.2\n",
            "Skipping invalid line: -5.4923077,-3.8307693,-2.6307693,-2.7230768,-2.6307693,-3.1846154,-2.353846,-2.5384614,-4.292308,-5.123077,-6.230769,-6.9692307,-8.2615385,-9.461538,-10.476923,-9.83077,-9.461538,-8.723077,-8.446154,-10.661538,-10.476923,-9.923077,-10.015385,-12.507692,-12.7846155,-15.553846,-15.553846,-15.184615,-14.446154,-15.553846,-13.061539,-12.6,-12.138461,-14.353847,-14.076923,-13.707692,-13.615385,-12.323077,-13.338462,-11.2153845,-11.492308,-8.907692,-8.538462,-7.3384614,-7.6153846,-5.8615384,-4.753846,-5.6769233,-3.8307693,-3.5538461,-4.6615386,-4.292308,-2.353846,-3.7384615,-2.1692307,-0.7846154,1.2461538,0.8769231,-1.7076923,-3.4615386,-5.123077,-4.6615386,-7.0615387,-6.415385,-7.1538463,-6.5076923,-6.876923,-5.953846,-4.753846,-1.8,0.50769234,1.4307692,2.1692307,0.8769231,3.3692307,2.9076922,3.9230769,4.1076922,1.4307692,-0.046153847,-0.046153847,-1.2461538,-1.3384615,1.8923076,2.6307693,1.8923076,1.8923076,3.3692307,4.3846154,5.953846,5.3076925,4.0153847,-0.046153847,-1.8923\n",
            "Number of classes: 5\n",
            "X_train shape: torch.Size([55802, 178, 1]), y_train shape: torch.Size([55802])\n",
            "X_valid shape: torch.Size([29162, 178, 1]), y_valid shape: torch.Size([29162])\n",
            "X_test shape: torch.Size([29162, 178, 1]), y_test shape: torch.Size([29162])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Function to load the .ts file\n",
        "def load_ts_file(file_path):\n",
        "    data = []\n",
        "    labels = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        is_metadata = True\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if is_metadata:\n",
        "                if line.lower() == \"@data\":\n",
        "                    is_metadata = False\n",
        "                continue\n",
        "            if len(line) > 0:\n",
        "                try:\n",
        "                    series, label = line.split(':')\n",
        "                    series = np.array(series.split(','), dtype=np.float32)\n",
        "                    data.append(series)\n",
        "                    labels.append(int(label))  # Convert the label to an integer\n",
        "                except ValueError:\n",
        "                    print(f\"Skipping invalid line: {line}\")\n",
        "                    continue\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "# Paths to the .ts files (update with your paths)\n",
        "train_data_path = '/content/Sleep_TEST.ts'\n",
        "test_data_path = '/content/Sleep_TEST.ts'\n",
        "\n",
        "# Load the train and test datasets\n",
        "X_train, y_train = load_ts_file(train_file_path)\n",
        "X_test, y_test = load_ts_file(test_file_path)\n",
        "\n",
        "# Normalize the features (optional)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Reshape the features to have a third dimension (samples, time_steps, 1)\n",
        "X_train_scaled = X_train_scaled.reshape(-1, X_train_scaled.shape[1], 1)\n",
        "X_test_scaled = X_test_scaled.reshape(-1, X_test_scaled.shape[1], 1)\n",
        "\n",
        "# Split the test data into validation and test sets\n",
        "X_valid_scaled, X_test_scaled, y_valid, y_test = train_test_split(X_test_scaled, y_test, test_size=0.50, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.int64)\n",
        "\n",
        "X_valid_tensor = torch.tensor(X_valid_scaled, dtype=torch.float32)\n",
        "y_valid_tensor = torch.tensor(y_valid, dtype=torch.int64)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.int64)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Calculate the number of unique classes\n",
        "n_classes = len(np.unique(y_train))\n",
        "\n",
        "# Print the shapes and number of classes\n",
        "print(f\"Number of classes: {n_classes}\")\n",
        "print(f\"X_train shape: {X_train_tensor.shape}, y_train shape: {y_train_tensor.shape}\")\n",
        "print(f\"X_valid shape: {X_valid_tensor.shape}, y_valid shape: {y_valid_tensor.shape}\")\n",
        "print(f\"X_test shape: {X_test_tensor.shape}, y_test shape: {y_test_tensor.shape}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
